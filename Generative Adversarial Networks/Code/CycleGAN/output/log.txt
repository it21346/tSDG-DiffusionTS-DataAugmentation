PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  True
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1
Loading data...

Creating model...

Epoch 0 [100/6287] loss_D: 0.3656 loss_G: 7.0511 time: 88.34
Epoch 0 [200/6287] loss_D: 0.2333 loss_G: 7.0465 time: 78.65
Epoch 0 [300/6287] loss_D: 0.3112 loss_G: 4.5877 time: 78.61
Epoch 0 [400/6287] loss_D: 0.2248 loss_G: 4.4051 time: 79.64
Epoch 0 [500/6287] loss_D: 0.1876 loss_G: 4.2015 time: 79.83
Epoch 0 [600/6287] loss_D: 0.2028 loss_G: 3.5612 time: 79.94
Epoch 0 [700/6287] loss_D: 0.1214 loss_G: 6.4159 time: 80.17
Epoch 0 [800/6287] loss_D: 0.4080 loss_G: 4.4955 time: 79.62
Epoch 0 [900/6287] loss_D: 0.3973 loss_G: 4.1580 time: 79.17
Epoch 0 [1000/6287] loss_D: 0.3557 loss_G: 5.4736 time: 79.10
Epoch 0 [1100/6287] loss_D: 0.3787 loss_G: 4.0591 time: 78.73
Epoch 0 [1200/6287] loss_D: 0.3383 loss_G: 3.1312 time: 78.76
Epoch 0 [1300/6287] loss_D: 0.2016 loss_G: 3.6899 time: 78.99
Epoch 0 [1400/6287] loss_D: 0.2013 loss_G: 3.2415 time: 78.75
Epoch 0 [1500/6287] loss_D: 0.4259 loss_G: 4.2631 time: 78.87
Epoch 0 [1600/6287] loss_D: 0.1015 loss_G: 3.3980 time: 78.79
Epoch 0 [1700/6287] loss_D: 0.0903 loss_G: 4.6490 time: 78.78
Epoch 0 [1800/6287] loss_D: 0.1556 loss_G: 3.7602 time: 78.76
Epoch 0 [1900/6287] loss_D: 0.2360 loss_G: 5.2156 time: 78.77
Epoch 0 [2000/6287] loss_D: 0.3020 loss_G: 4.3665 time: 78.74
Epoch 0 [2100/6287] loss_D: 0.0912 loss_G: 3.7190 time: 78.76
Epoch 0 [2200/6287] loss_D: 0.2204 loss_G: 3.2003 time: 78.73
Epoch 0 [2300/6287] loss_D: 0.2923 loss_G: 3.2203 time: 78.76
Epoch 0 [2400/6287] loss_D: 0.1161 loss_G: 3.5079 time: 79.00
Epoch 0 [2500/6287] loss_D: 0.1582 loss_G: 3.0974 time: 80.57
Epoch 0 [2600/6287] loss_D: 0.0926 loss_G: 4.6038 time: 79.33
Epoch 0 [2700/6287] loss_D: 0.2205 loss_G: 3.6912 time: 79.02
Epoch 0 [2800/6287] loss_D: 0.1330 loss_G: 3.2170 time: 79.01
Epoch 0 [2900/6287] loss_D: 0.2392 loss_G: 3.7719 time: 78.98
Epoch 0 [3000/6287] loss_D: 0.2301 loss_G: 3.2430 time: 79.09
Epoch 0 [3100/6287] loss_D: 0.2105 loss_G: 3.4665 time: 80.57
Epoch 0 [3200/6287] loss_D: 0.2023 loss_G: 2.7394 time: 79.41
Epoch 0 [3300/6287] loss_D: 0.2784 loss_G: 4.5183 time: 79.89
Epoch 0 [3400/6287] loss_D: 0.2594 loss_G: 4.9892 time: 79.93
Epoch 0 [3500/6287] loss_D: 0.3688 loss_G: 5.4210 time: 79.98
Epoch 0 [3600/6287] loss_D: 0.4176 loss_G: 3.1126 time: 79.51
Epoch 0 [3700/6287] loss_D: 0.0873 loss_G: 5.0257 time: 80.34
Epoch 0 [3800/6287] loss_D: 0.1669 loss_G: 3.6059 time: 80.33
Epoch 0 [3900/6287] loss_D: 0.1687 loss_G: 3.0955 time: 79.99
Epoch 0 [4000/6287] loss_D: 0.3144 loss_G: 3.6578 time: 81.78
Epoch 0 [4100/6287] loss_D: 0.4042 loss_G: 3.9672 time: 81.52
Epoch 0 [4200/6287] loss_D: 0.1124 loss_G: 2.9088 time: 79.96
Epoch 0 [4300/6287] loss_D: 0.2335 loss_G: 5.1609 time: 78.98
Epoch 0 [4400/6287] loss_D: 0.1621 loss_G: 4.0758 time: 78.85
Epoch 0 [4500/6287] loss_D: 0.2234 loss_G: 2.9740 time: 78.86
Epoch 0 [4600/6287] loss_D: 0.2945 loss_G: 2.6640 time: 78.87
Epoch 0 [4700/6287] loss_D: 0.2718 loss_G: 2.6809 time: 78.75
Epoch 0 [4800/6287] loss_D: 0.1013 loss_G: 3.1226 time: 79.53
Epoch 0 [4900/6287] loss_D: 0.2437 loss_G: 3.0853 time: 79.46
Epoch 0 [5000/6287] loss_D: 0.1866 loss_G: 2.5512 time: 80.55
Epoch 0 [5100/6287] loss_D: 0.0990 loss_G: 2.5107 time: 80.30
Epoch 0 [5200/6287] loss_D: 0.2514 loss_G: 3.0684 time: 78.91
Epoch 0 [5300/6287] loss_D: 0.2134 loss_G: 3.1451 time: 79.05
Epoch 0 [5400/6287] loss_D: 0.2207 loss_G: 3.4185 time: 79.93
Epoch 0 [5500/6287] loss_D: 0.2493 loss_G: 3.4204 time: 78.97
Epoch 0 [5600/6287] loss_D: 0.2291 loss_G: 3.3239 time: 79.71
Epoch 0 [5700/6287] loss_D: 0.3504 loss_G: 3.1796 time: 79.88
Epoch 0 [5800/6287] loss_D: 0.1629 loss_G: 3.4776 time: 79.53
Epoch 0 [5900/6287] loss_D: 0.1995 loss_G: 3.3228 time: 79.38
Epoch 0 [6000/6287] loss_D: 0.2343 loss_G: 3.5890 time: 81.23
Epoch 0 [6100/6287] loss_D: 0.1482 loss_G: 2.5569 time: 79.75
Epoch 0 [6200/6287] loss_D: 0.1507 loss_G: 2.9149 time: 80.95
Epoch 1 [100/6287] loss_D: 0.2992 loss_G: 2.4856 time: 86.29
Epoch 1 [200/6287] loss_D: 0.0597 loss_G: 3.3217 time: 79.33
Epoch 1 [300/6287] loss_D: 0.0857 loss_G: 3.9469 time: 79.42
Epoch 1 [400/6287] loss_D: 0.1080 loss_G: 2.7131 time: 79.82
Epoch 1 [500/6287] loss_D: 0.3255 loss_G: 2.6584 time: 78.99
Epoch 1 [600/6287] loss_D: 0.1641 loss_G: 3.0381 time: 78.90
Epoch 1 [700/6287] loss_D: 0.1316 loss_G: 2.9880 time: 79.19
Epoch 1 [800/6287] loss_D: 0.1790 loss_G: 2.6643 time: 80.56
Epoch 1 [900/6287] loss_D: 0.1088 loss_G: 2.4384 time: 80.55
Epoch 1 [1000/6287] loss_D: 0.2440 loss_G: 2.4016 time: 80.96
Epoch 1 [1100/6287] loss_D: 0.2437 loss_G: 2.3073 time: 80.75
Epoch 1 [1200/6287] loss_D: 0.2050 loss_G: 2.8697 time: 79.35
Epoch 1 [1300/6287] loss_D: 0.2231 loss_G: 3.6925 time: 82.63
Epoch 1 [1400/6287] loss_D: 0.2059 loss_G: 4.2576 time: 80.43
Epoch 1 [1500/6287] loss_D: 0.2042 loss_G: 3.9036 time: 81.18
Epoch 1 [1600/6287] loss_D: 0.1516 loss_G: 3.3326 time: 81.04
Epoch 1 [1700/6287] loss_D: 0.0615 loss_G: 3.2405 time: 79.58
Epoch 1 [1800/6287] loss_D: 0.1411 loss_G: 2.6648 time: 78.96
Epoch 1 [1900/6287] loss_D: 0.1353 loss_G: 3.1880 time: 79.02
Epoch 1 [2000/6287] loss_D: 0.1941 loss_G: 2.9734 time: 80.70
Epoch 1 [2100/6287] loss_D: 0.2304 loss_G: 2.6201 time: 79.46
Epoch 1 [2200/6287] loss_D: 0.2620 loss_G: 3.0475 time: 80.07
Epoch 1 [2300/6287] loss_D: 0.1996 loss_G: 2.6505 time: 80.99
Epoch 1 [2400/6287] loss_D: 0.1135 loss_G: 3.6356 time: 84.24
Epoch 1 [2500/6287] loss_D: 0.0677 loss_G: 3.7136 time: 82.51
Epoch 1 [2600/6287] loss_D: 0.2409 loss_G: 2.8022 time: 80.52
Epoch 1 [2700/6287] loss_D: 0.0930 loss_G: 2.4240 time: 79.09
Epoch 1 [2800/6287] loss_D: 0.1963 loss_G: 2.9310 time: 78.99
Epoch 1 [2900/6287] loss_D: 0.1956 loss_G: 2.5012 time: 78.93
Epoch 1 [3000/6287] loss_D: 0.3223 loss_G: 2.7896 time: 78.96
Epoch 1 [3100/6287] loss_D: 0.0797 loss_G: 3.1669 time: 79.53
Epoch 1 [3200/6287] loss_D: 0.5135 loss_G: 2.6449 time: 79.14
Epoch 1 [3300/6287] loss_D: 0.0491 loss_G: 2.5991 time: 81.20
Epoch 1 [3400/6287] loss_D: 0.1612 loss_G: 3.1835 time: 80.45
Epoch 1 [3500/6287] loss_D: 0.1467 loss_G: 3.3103 time: 80.93
Epoch 1 [3600/6287] loss_D: 0.3182 loss_G: 1.8638 time: 79.38
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1

Loading models from cyclegan_G_AB.pt and such ...
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1

Loading models from cyclegan_G_AB.pt and such ...
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
torch.Size([4, 3, 256, 256])
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1

Loading models from cyclegan_G_AB.pt and such ...
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1

Loading models from cyclegan_G_AB.pt and such ...
(1, 3, 256, 256)
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1

Loading models from cyclegan_G_AB.pt and such ...
(1, 3, 256, 256)
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1

Loading models from cyclegan_G_AB.pt and such ...
torch.Size([1, 3, 256, 256])
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1

Loading models from cyclegan_G_AB.pt and such ...
torch.Size([1, 3, 256, 256])
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1

Loading models from cyclegan_G_AB.pt and such ...
torch.Size([1, 3, 256, 256])
PyTorch version: 2.2.2+cu121
CUDA version: 12.1

         Args         |    Type    |    Value
--------------------------------------------------
  model               |  str       |  cyclegan
  cuda                |  bool      |  True
  train               |  bool      |  False
  data_dir            |  str       |  ../Datasets/vangogh2photo
  dataset             |  str       |  vangogh2photo
  out_dir             |  str       |  output
  epochs              |  int       |  5
  batch_size          |  int       |  1
  test_batch_size     |  int       |  4
  lr                  |  float     |  0.0002
  img_size            |  int       |  256
  channels            |  int       |  3
  num_blocks          |  int       |  9
  log_interval        |  int       |  100
  seed                |  int       |  1

Loading models from cyclegan_G_AB.pt and such ...
torch.Size([1, 3, 256, 256])
