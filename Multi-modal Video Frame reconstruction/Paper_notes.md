| Paper | Synthetic Generation | Multi-modal | Physics-driven | Notes |
| :---  |  :----:   |    :----:   | :----:   | ---: |
| [Wang_Event-Driven_Video_Frame_Synthesis_ICCVW_2019_paper (two sensing modalities).pdf](Wang_Event-Driven_Video_Frame_Synthesis_ICCVW_2019_paper%20%28two%20sensing%20modalities%29.pdf) | [**X**] | [**X**] | [] | This paper introduces a novel high frame-rate video synthesis framework by fusing intensity frames with event streams. They use a diffusion model and also introduce a resideual denoiser for the outputs from that model. They show that their framework is capable of handling challenging scenes, including both fast motion and strong occlusions. |
| [Paper that shows multi-modal sensor fusion with scene understanding.pdf](Paper%20that%20shows%20multi-modal%20sensor%20fusion%20with%20scene%20understanding.pdf) | [] | [**X**] | [] | This paper focuses on utilizing multimodal information (image and depth) for an end-to-end autonomous driving. This paper does not use SDG or GANs but is solely interesting because of its multi-modal fusion. They introduce a network, where the input is the RGB image (h x w x 3) and depth image (h x w x 1) and they pass this input through a sensor fusion encoder where they output a feature map. This feature map then is used as input in a scene understanding decoder, which in their case is a pixel-wise segmentation mask. In essence, they use this information for the model to predict speed and steering angle in the end. It is an interesting paper regarding autonomous driving, but mainly the multi-modal fusion section.|
| [Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.pdf](Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.pdf) | [**X**] | [**X**] | [] | This paper was very interesting and focusing on generating videos by mapping a sequence of random vectors to a sequence of video frames. They decomposite the random vectors into two parts, the content and motion part. The content part is for example a person, a face, basically static things that shouldnt change much in small video clips. In contrast, the motion part is about actions, movement inside small video clips which usually change. Thus, for base cases they keep the content fixed and have the motion part as a stochastic process. To achieve this decomposition while unsupervised, they utilize the GAN framework by having a Generator, a RNN for mapping a sequence of i.i.d random variables to a sequence of correlated random variables representing the dynamics in a video, and two discriminators, one for frames and one for sequence of frames. They claim they achieved better results than other SOTA models in 4 different datasets. This paper was particularly interesting and helpful in understanding how they treat frames vs sequence of frames, how they decomposite the motion and content from videos and how they train all this pipeline while having GAN-based models as a framework.|
| [Deep_Learning_in_Next-Frame_Prediction_A_Benchmark_Review.pdf](Deep_Learning_in_Next-Frame_Prediction_A_Benchmark_Review.pdf) | [**X**] | [] | [] | This paper is a review paper from 2020. It delves into the two different segmentations they do for sequencing and next-frame prediction, the sequence-to-1 and sequence-to-sequence predictions. The first uses k previous frames and predicts n frames. The latter, use the predicted frames for the next prediction, consequently. I don't think they take much into consideration the GAN architecture as a sole focus, and also the video frame generation. It definitely doesn't touch the multi-modal and physics process. | 
| [Generative_Adversarial_Network-Based_Frame_Interpolation_with_Multi-Perspective_Discrimination.pdf](Generative_Adversarial_Network-Based_Frame_Interpolation_with_Multi-Perspective_Discrimination.pdf) | [**X**] | [] | [] | This paper introduced a frame interpolation framework utilizing three adversarial networks, including a three-scale generator and two discriminators. The devised generator captures coarse-to-fine visual characteristics by three-scale sub-modules with an adequate combination of loss functions. Besides a discriminator considering the quality of generated  data, a temporal one is employed to guarantee consistency among adjacent frames. One key mention here is that this paper focuses on the interpolation process, meaning it procudes intermediate frames.|
| [PlantPlotGAN_(Paper that incorporates physics into the discriminators in a GAN Framework).pdf](PlantPlotGAN_%28Paper%20that%20incorporates%20physics%20into%20the%20discriminators%20in%20a%20GAN%20Framework%29.pdf) | [**X**] | [] | [**X**] | This paper introduces physics constraints for synthetic generation. This is based for images and not videos, also they  parametrize the latent space before passing it into the generator.  They make a spectrum regularizer  that factorizes the latent space with optimized spectral coefficients. They also use two discriminators each checking different things. This is a good paper, delving into the incorporation of physics-based generation. |
| [Video-to-Video Synthesis](1808.06601v2.pdf) | [**X**] | [**X**] | [] | This paper presents a general video-to-video synthesis, unlike previous attemps like image-to-image translations. It is based on GANs but they incorporate a spatio-temporal adversarial objective. Their model also performs well with video prediction, based on few previous frames. The different input formats they used are segmentation masks, sketches and poses. |
| [Video_Reconstruction_with_Multimodal_Information.pdf](Video_Reconstruction_with_Multimodal_Information.pdf) | [**X**] | [**X**] | [] | This paper proposes a novel approach that generates realistic video from its multimodal information, including structure features (edge maps) and color features. To extract the color features, they used the k-means algorithm to segment the labels and the edge maps are extracted by an edge detection network. They compare their results mostly with vid2vid architectures showing that their model actually captures better color matching on the synthesized frames. |
| [DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with Video Diffusion Priors](2406.01476v1.pdf) | [**X**] | [] | [**X**] | Its not about GANs, but Stable Video DIffusion.|
| [NIPS-2016-generating-videos-with-scene-dynamics-Paper.pdf](NIPS-2016-generating-videos-with-scene-dynamics-Paper.pdf) | [**X**] | [**X**] | [] | This paper introduces a way to generate videos, by having two streams, one to extract the foreground and one to extract the background. They dont use multi-modal or physics constrains.|
| [MOTIONCRAFT: Physics-based Zero-Shot Video Generation](2405.13557v1.pdf) | [] | [] | [] |  |
| [PETIT-GAN_Physically_Enhanced_Thermal_Image-Translating_Generative_Adversarial_Network.pdf](PETIT-GAN_Physically_Enhanced_Thermal_Image-Translating_Generative_Adversarial_Network.pdf) | [] | [] | [] |  |
| [1910.12713v1.pdf](1910.12713v1.pdf) | [] | [] | [] |  |
| [2009.01689v1.pdf](2009.01689v1.pdf) | [] | [] | [] |  |
| [DCVGAN_Depth_Conditional_Video_Generation.pdf](DCVGAN_Depth_Conditional_Video_Generation.pdf) | [] | [] | [] |  |
| [1804.01523v1.pdf](1804.01523v1.pdf) | [] | [] | [] |  |
| [Physics-Driven_Diffusion_Models_for_Impact_Sound_Synthesis_from_Videos.pdf](Physics-Driven_Diffusion_Models_for_Impact_Sound_Synthesis_from_Videos.pdf) | [] | [] | [] |  |
| [icml11-MultimodalDeepLearning(shows how multimodal is better for the network).pdf](icml11-MultimodalDeepLearning%28shows%20how%20multimodal%20is%20better%20for%20the%20network%29.pdf) | [] | [] | [] |  |
| [Pumarola_D-NeRF_Neural_Radiance_Fields_for_Dynamic_Scenes_CVPR_2021_paper.pdf](Pumarola_D-NeRF_Neural_Radiance_Fields_for_Dynamic_Scenes_CVPR_2021_paper.pdf) | [] | [] | [] |  |
