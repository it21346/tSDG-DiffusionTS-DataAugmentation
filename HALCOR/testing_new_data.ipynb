{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_csv(path, name):\n",
    "    all_files = glob.glob(path + \"*.csv\")\n",
    "    if name not in[\"exodos\", \"junker2\"]: # the exodos folder seem to contain weird symbols for some column names, thus latin-1 encoding is needed\n",
    "        dfs = [pd.read_csv(file, sep=';') for file in all_files]\n",
    "    else:\n",
    "        dfs = [pd.read_csv(file, sep=';', encoding='latin-1') for file in all_files]\n",
    "    combined_df = pd.concat(dfs, join = 'outer', ignore_index=True)\n",
    "    # Combine 'Datum' and 'Absolutezeit' into a single datetime column\n",
    "    combined_df['Timestamp'] = pd.to_datetime(combined_df['Datum'] + ' ' + combined_df['Absolutzeit'],  dayfirst=True)\n",
    "    # Sort the concatenated DataFrame by the new 'Timestamp' column\n",
    "    sorted_df = combined_df.sort_values(by='Timestamp')\n",
    "    # Save the result to a new CSV if needed\n",
    "    sorted_df.to_csv(name + \".csv\", sep = ';', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Auto Press folder (inclusive join. High disparity in column lengths among the csv's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./New_HalcoR_Data(2024)/OneDrive_1_12-5-2024/Auto Press/\"\n",
    "unify_csv(folder_path, \"autopress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574747/3949148293.py:2: DtypeWarning: Columns (6,76,88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"autopress.csv\", sep = ';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates ranging from 2023-12-07 08:41:41 to 2024-11-11 10:53:00\n"
     ]
    }
   ],
   "source": [
    "# Date range\n",
    "df = pd.read_csv(\"autopress.csv\", sep = ';')\n",
    "print(\"Dates ranging from \" + df['Timestamp'][0] + \" to \" + df['Timestamp'][-1:].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Antlies Pressas folder (inclusive join. High disparity in column lengths among the csv's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574747/1463582008.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(dfs, join = 'outer', ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./New_HalcoR_Data(2024)/OneDrive_1_12-5-2024/Antlies Pressas/\"\n",
    "unify_csv(folder_path, \"antliespressas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates ranging from 2024-01-02 11:42:59 to 2024-04-17 17:59:37\n"
     ]
    }
   ],
   "source": [
    "# Date range\n",
    "df = pd.read_csv(\"antliespressas.csv\", sep = ';')\n",
    "print(\"Dates ranging from \" + df['Timestamp'][0] + \" to \" + df['Timestamp'][-1:].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Exodos folder (inclusive join. High disparity in column lengths among the csv's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574747/1463582008.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(dfs, join = 'outer', ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./New_HalcoR_Data(2024)/OneDrive_1_12-5-2024/Exodos/\"\n",
    "unify_csv(folder_path, \"exodos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates ranging from 2024-06-06 08:31:51 to 2024-10-25 16:02:42\n"
     ]
    }
   ],
   "source": [
    "# Date range\n",
    "df = pd.read_csv(\"exodos.csv\", sep = ';')\n",
    "print(\"Dates ranging from \" + df['Timestamp'][0] + \" to \" + df['Timestamp'][-1:].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify InterlockAntliostasiou folder (inclusive join. High disparity in column lengths among the csv's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574747/1463582008.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(dfs, join = 'outer', ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./New_HalcoR_Data(2024)/OneDrive_1_12-5-2024/InterlockAntliostaiou/\"\n",
    "unify_csv(folder_path, \"interlockantliostaiou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates ranging from 2023-12-06 15:43:26 to 2024-06-03 07:20:11\n"
     ]
    }
   ],
   "source": [
    "# Date range\n",
    "df = pd.read_csv(\"interlockantliostaiou.csv\", sep = ';')\n",
    "print(\"Dates ranging from \" + df['Timestamp'][0] + \" to \" + df['Timestamp'][-1:].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Junker1 folder (inclusive join. High disparity in column lengths among the csv's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./New_HalcoR_Data(2024)/OneDrive_1_12-5-2024/Junker1/\"\n",
    "unify_csv(folder_path, \"junker1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574747/1628634808.py:2: DtypeWarning: Columns (103,104,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"junker1.csv\", sep = ';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates ranging from 2024-06-24 16:24:22 to 2024-10-24 11:09:14\n"
     ]
    }
   ],
   "source": [
    "# Date range\n",
    "df = pd.read_csv(\"junker1.csv\", sep = ';')\n",
    "print(\"Dates ranging from \" + df['Timestamp'][0] + \" to \" + df['Timestamp'][-1:].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Junker2 folder (inclusive join. High disparity in column lengths among the csv's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./New_HalcoR_Data(2024)/OneDrive_1_12-5-2024/Junker2/\"\n",
    "unify_csv(folder_path, \"junker2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574747/3174954517.py:2: DtypeWarning: Columns (108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"junker2.csv\", sep = ';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates ranging from 2024-06-28 07:55:20 to 2024-10-25 12:12:12\n"
     ]
    }
   ],
   "source": [
    "# Date range\n",
    "df = pd.read_csv(\"junker2.csv\", sep = ';')\n",
    "print(\"Dates ranging from \" + df['Timestamp'][0] + \" to \" + df['Timestamp'][-1:].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Energy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_313725/4119017451.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n",
      "/tmp/ipykernel_313725/4119017451.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n",
      "/tmp/ipykernel_313725/4119017451.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n",
      "/tmp/ipykernel_313725/4119017451.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n",
      "/tmp/ipykernel_313725/4119017451.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n",
      "/tmp/ipykernel_313725/4119017451.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n",
      "/tmp/ipykernel_313725/4119017451.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n",
      "/tmp/ipykernel_313725/4119017451.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n",
      "/tmp/ipykernel_313725/4119017451.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all energies together for each timestamp\n",
    "total_energy = None\n",
    "# Folder with energy consumption files\n",
    "path_to_energy_consumption_files = './PRESS PME DATA FY2024.xlsx'\n",
    "excel_data = pd.ExcelFile(path_to_energy_consumption_files)\n",
    "start_timestamp = pd.Timestamp('2024-01-01 11:30:00')\n",
    "df_energy = []\n",
    "# Loop through each sheet\n",
    "for sheet_name in excel_data.sheet_names:\n",
    "    # Read the sheet\n",
    "    df = excel_data.parse(sheet_name)\n",
    "    # Keep only the first two columns\n",
    "    df = df.iloc[:, :2]\n",
    "    # Make all the sheets start from a specific timestamp for smoothness. Convert the timestamp column to datetime (assume it's the first column)\n",
    "    df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])\n",
    "    df = df[df.iloc[:, 0] >= start_timestamp]\n",
    "    df['Real Energy'] = pd.to_numeric(df['Real Energy'], downcast='float', errors='coerce')\n",
    "    # Adjust timestamps where seconds are not 00\n",
    "    df['Timestamp'] = df['Timestamp'].apply(lambda x: x.replace(second=0))\n",
    "    df = df.sort_values(by='Timestamp', ascending= True)\n",
    "    # Remove duplicates that exist in the dataset, if they exist.\n",
    "    df.drop_duplicates(subset='Timestamp', keep='last', inplace = True)\n",
    "    # the '-' rows should be filled with the previous value instead (the previous 15-minute)\n",
    "    df = df.applymap(lambda x : np.nan if x == '-' else x) # convert - into nan so later can be filled with 'fillna' later\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "    df = df.resample('15min', closed='right', label='right').ffill()\n",
    "    df.reset_index(inplace=True)\n",
    "    df['Real Energy'] = df['Real Energy'].interpolate() # interpolate values\n",
    "    # Append the DataFrame to the list\n",
    "    df_energy.append(df)\n",
    "\n",
    "\n",
    "total_energy = pd.concat(df_energy, ignore_index=True)\n",
    "total_energy = total_energy.groupby('Timestamp').sum().reset_index()\n",
    "total_energy.set_index('Timestamp', inplace=True)\n",
    "# Set the initial granularity to 15 minute intervals for baseline.\n",
    "total_energy = total_energy.resample('15min', closed='right', label='right').ffill()\n",
    "# total_energy.replace(0, np.nan, inplace=True)\n",
    "# total_energy = total_energy.interpolate()\n",
    "total_energy.reset_index(inplace=True)\n",
    "\n",
    "# Since energy was cumulative, We take the difference and save it into a column\n",
    "total_energy['diff'] = total_energy['Real Energy'].diff()\n",
    "total_energy['diff'] = total_energy['diff'].interpolate()\n",
    "total_energy['Timestamp'] = pd.to_datetime(total_energy['Timestamp'])\n",
    "total_energy = total_energy.iloc[:-1]\n",
    "total_energy.drop(columns='Real Energy', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot energy consumption over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_energy['Timestamp'], total_energy['diff'], marker='o', linestyle='-', color='b', label='Energy Consumption')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Energy Consumption Over Time', fontsize=16)\n",
    "plt.xlabel('Timestamp', fontsize=14)\n",
    "plt.ylabel('Energy Consumption', fontsize=14)\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the grid\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_energy['Timestamp'] = total_energy['Timestamp'].dt.strftime('%d/%m/%Y %H:%M:%S')\n",
    "total_energy.to_excel(\"temp.xlsx\", engine='openpyxl', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
