{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366770e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from math import sqrt\n",
    "from os.path import exists\n",
    "# Initialize Wandb\n",
    "import wandb\n",
    "print(wandb.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "from numpy import concatenate\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Bidirectional, Dense, Dropout, LSTM, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Embedding, Input, LSTM\n",
    "import os, random, tensorflow as tf\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "ori_data = pd.read_csv(\"Updated_Raw_Halcor_Data/All_Data_Reducted.csv\", sep = ';')\n",
    "ori_data['Timestamp'] = pd.to_datetime(ori_data[\"Timestamp\"])\n",
    "# this yields a better test set, as the last 140 samples are basically on a weekend and the energy consumption is very low, so it is not representative of the data and the metrics are inflated\n",
    "ori_data = ori_data[:-120] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the energy consumption\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ori_data['Timestamp'], ori_data['Energy'], label='Energy Consumption', color='blue')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Energy Consumption Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('energy_consumption_plot.png', dpi=600, format='png')  # Save the energy consumption plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729599a9",
   "metadata": {},
   "source": [
    "# Plot the heat map of original data based on day of week and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cfe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heat map of original data based on day of week and hour and delete the newly added columns\n",
    "\n",
    "def plot_heatmap(data):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of the original data based on day of week and hour.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing the original data with 'Timestamp' column.\n",
    "    \"\"\"\n",
    "    # Extract day of week and hour from the timestamp\n",
    "    data['day_of_week'] = data['Timestamp'].dt.dayofweek\n",
    "    data['hour'] = data['Timestamp'].dt.hour\n",
    "    \n",
    "    # Create a pivot table for the heatmap\n",
    "    heatmap_data = data.pivot_table(index='day_of_week', columns='hour', values='Energy', aggfunc='mean')\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title('Heatmap of Original Data by Day of Week and Hour')\n",
    "    plt.imshow(heatmap_data, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Mean Value')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Day of Week (0=Monday, 6=Sunday)')\n",
    "    plt.xticks(ticks=np.arange(24), labels=np.arange(24))\n",
    "    plt.yticks(ticks=np.arange(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Drop the newly added columns\n",
    "    data.drop(columns=['day_of_week', 'hour'], inplace=True)\n",
    "\n",
    "plot_heatmap(ori_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c7e11",
   "metadata": {},
   "source": [
    "# Plot the ACF (auto-correlation function) and PACF (partial auto-correlation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b94a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "\n",
    "series = ori_data['Energy']\n",
    "\n",
    "# 1) Autocorrelation Function\n",
    "plt.figure()\n",
    "plot_acf(series, lags=64)        # adjust lags as needed\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# 2) Partial Autocorrelation Function\n",
    "plt.figure()\n",
    "plot_pacf(series, lags=64, method='ywm')  # you can also try method='ld' or 'ywmle'\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73699610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(data):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    return scaled_data, scaler\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    return scaled_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_model(X_train, Y_train, X_val, Y_val, X_test, Y_test, seed_val=42, pinball_loss_usage=False):\n",
    "    \"\"\"\n",
    "    Trains XGBoost quantile regression models for 5%, 50%, 95% intervals.\n",
    "    \"\"\"\n",
    "\n",
    "    quantiles = [0.05, 0.5, 0.95]\n",
    "    models = {}\n",
    "    predictions = {}\n",
    "    X_test = X_test[48:-1]\n",
    "    Y_test = Y_test[48:-1]\n",
    "    if pinball_loss_usage:\n",
    "        for q in quantiles:\n",
    "            train_data = xgb.QuantileDMatrix(X_train, label=Y_train)\n",
    "            val_data = xgb.QuantileDMatrix(X_val, label=Y_val)\n",
    "            test_data = xgb.QuantileDMatrix(X_test, label=Y_test)\n",
    "\n",
    "            params = {\n",
    "                'objective': 'reg:quantileerror',\n",
    "                'quantile_alpha': q,\n",
    "                'eval_metric': 'mae',\n",
    "                'learning_rate': 0.01,\n",
    "                'max_depth': 5,\n",
    "                'gamma': 1.0,\n",
    "                'alpha': 0.1,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'colsample_bynode': 0.8,\n",
    "                'lambda': 0.1,\n",
    "                'tree_method': 'hist',  # Required for QuantileDMatrix\n",
    "                'seed': seed_val,\n",
    "            }\n",
    "\n",
    "            evals = [(train_data, 'train'), (val_data, 'eval')]\n",
    "            model = xgb.train(params, train_data, num_boost_round=2000, early_stopping_rounds=100, evals=evals, verbose_eval=False)\n",
    "\n",
    "            models[q] = model\n",
    "            predictions[q] = model.predict(test_data)\n",
    "        print(\"Shape of Y_test: \", Y_test.shape)\n",
    "        return {\n",
    "            'models': models,\n",
    "            'predictions': predictions,\n",
    "            'y_true': Y_test\n",
    "        }\n",
    "    else:\n",
    "        train_data = xgb.DMatrix(X_train, label=Y_train)\n",
    "        val_data = xgb.DMatrix(X_val, label=Y_val)\n",
    "        test_data = xgb.DMatrix(X_test, label=Y_test)\n",
    "\n",
    "        params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                # 'quantile_alpha': q,\n",
    "                'eval_metric': 'rmse',\n",
    "                'learning_rate': 0.01,\n",
    "                'max_depth': 5,\n",
    "                'gamma': 1.0,\n",
    "                'alpha': 0.1,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'colsample_bynode': 0.8,\n",
    "                'lambda': 0.1,\n",
    "                'tree_method': 'hist',  # Required for QuantileDMatrix\n",
    "                'seed': seed_val,\n",
    "            }\n",
    "\n",
    "        evals = [(train_data, 'train'), (val_data, 'eval')]\n",
    "        model = xgb.train(params, train_data, num_boost_round=1000, early_stopping_rounds=100, evals=evals, verbose_eval=False)\n",
    "        predictions = model.predict(test_data)\n",
    "        return predictions, model\n",
    "def RF_model(X_train, Y_train, seed_val=42):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=seed_val)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "def quantile_loss(q):\n",
    "    def loss(y_true, y_pred):\n",
    "        e = y_true - y_pred\n",
    "        return tf.reduce_mean(tf.maximum(q * e, (q - 1) * e))\n",
    "    return loss\n",
    "\n",
    "def BiLSTM_model(X_train, q, pinball_loss_usage=False):\n",
    "        \"\"\"Normalize all features to zero mean and unit variance (StandardScaler) after cutting your windows.\n",
    "\n",
    "\n",
    "        Constructing the Bidirectional LSTM architecture.\n",
    "\n",
    "        Returns:\n",
    "            model (keras.model) : Returns the keras model.\n",
    "        \"\"\"\n",
    "        # lr_schedule = ExponentialDecay(0.001, decay_steps=2500, decay_rate=0.96, staircase=True)\n",
    "        model = Sequential()\n",
    "        # model.add(Bidirectional(LSTM(50, activation='tanh', return_sequences=True, dropout= 0.1, recurrent_dropout= 0.2, input_shape=(X_train.shape[1], X_train.shape[2])), merge_mode='concat'))\n",
    "        model.add(Bidirectional(LSTM(128, activation='tanh', return_sequences=True, dropout= 0.1, recurrent_dropout=0.2,input_shape=(X_train.shape[1], X_train.shape[2]))))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Bidirectional(LSTM(128, activation='tanh', dropout= 0.1, recurrent_dropout=0.2, return_sequences=False)))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        optimizer = Adam(learning_rate= 0.004)\n",
    "        if pinball_loss_usage:\n",
    "            model.compile(loss=quantile_loss(q), optimizer=optimizer, metrics=['mae', 'mape', 'mse'])\n",
    "        else:\n",
    "            model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mape'])\n",
    "        return model\n",
    "\n",
    "\n",
    "def LSTM_model(X_train, q, pinball_loss_usage=False):\n",
    "    # lr_schedule = ExponentialDecay(0.001, decay_steps=2500, decay_rate=0.96, staircase=True)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation= 'tanh', return_sequences=True, dropout= 0.1, recurrent_dropout=0.2, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, activation= 'tanh', recurrent_dropout=0.2, dropout= 0.1, return_sequences=False))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    # optimizer = Adam(learning_rate=lr_schedule)\n",
    "    if pinball_loss_usage:\n",
    "        model.compile(loss=quantile_loss(q), optimizer='Adam', metrics=['mae', 'mape', 'mse'])\n",
    "    else:\n",
    "        model.compile(loss='mse', optimizer='Adam', metrics=['mae', 'mape'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\n",
    "\n",
    "# Baseline model\n",
    "def Exp_Smoothing_model(train):\n",
    "    \"\"\"\n",
    "    Constructing the Exponential Smoothing model.\n",
    "\n",
    "    Returns:\n",
    "        model (statsmodels.tsa.holtwinters.ExponentialSmoothing) : Returns the fitted model.\n",
    "    \"\"\"\n",
    "    # Fit the model\n",
    "    model = ExponentialSmoothing(train, trend='add', seasonal='add', damped_trend=True, seasonal_periods=48).fit()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Baseline model\n",
    "data = ori_data[['Timestamp','Energy']].copy()\n",
    "# Ensure 'Timestamp' is the index\n",
    "data.set_index('Timestamp', inplace=True)\n",
    "\n",
    "series = data['Energy']\n",
    "n = len(series)\n",
    "train_end = int(n * 0.85)\n",
    "val_end = train_end + int(n * 0.075)\n",
    "train = series[:train_end]\n",
    "val   = series[train_end:val_end]\n",
    "test  = series[val_end:]\n",
    "model = Exp_Smoothing_model(train)\n",
    "\n",
    "forecast_steps = len(test)\n",
    "forecast = model.forecast(forecast_steps)\n",
    "forecast.index = test.index\n",
    "\n",
    "shape2 = len(test)\n",
    "mae = mean_absolute_error(test, forecast)\n",
    "rmse = np.sqrt(mean_squared_error(test, forecast))\n",
    "r2 = r2_score(test, forecast)\n",
    "n = len(test)\n",
    "p = shape2\n",
    "# MASE function\n",
    "def mase(y_true, y_pred, y_naive):\n",
    "    # y_true: Actual values\n",
    "    # y_pred: Predicted values\n",
    "    # y_naive: Naive model predictions (e.g., previous period's values)\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_naive = np.asarray(y_naive)\n",
    "    # Compute errors\n",
    "    errors = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Calculate MAE of naive model\n",
    "    naive_errors = np.abs(y_true[1:] - y_naive[1:])\n",
    "    mae_naive = np.mean(naive_errors)\n",
    "    \n",
    "    # Calculate MASE\n",
    "    mase_value = np.mean(errors) / mae_naive\n",
    "    return mase_value\n",
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    # y_true: Actual values\n",
    "    # y_pred: Predicted values\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    # Calculate SMAPE\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    smape_value = 100 * np.mean(diff)\n",
    "    return smape_value\n",
    "\n",
    "smape_val = smape(test, forecast)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Mean squared error (MSE): {mean_squared_error(test, forecast)}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'R² Score: {r2}')\n",
    "print(f\"SMAPE score: {smape_val}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.plot(forecast, label='prediction', color=\"r\")\n",
    "plt.plot(test, label='actual', marker='.')\n",
    "plt.legend()\n",
    "plt.ylabel('Energy', size=15)\n",
    "plt.xlabel('Time step', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(seed_val = 42, model_type = 'BiLSTM', n_past = 48, batch_size = 32, scaler_name = 'min-max', data_augmentation = False, fake_data_length = 400, testing_stage = False, pinball_loss_usage = False):\n",
    "    # Define seed for reproducibility\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_val)\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    tf.random.set_seed(seed_val)\n",
    "\n",
    "    data = ori_data.copy()\n",
    "    n_past = n_past # the ACF shows around 50 lags, so we can use 48 -> 12 hours of lookback\n",
    "    n_future = 1\n",
    "\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "\n",
    "    # Window sliding technique used for Deep Learning models\n",
    "    def split_sequence(X, Y, steps, out):\n",
    "        Xs, Ys = list(), list()\n",
    "        for i in range(len(X)):\n",
    "            end = i + steps\n",
    "            outi = end + out\n",
    "            if outi > len(X)-1:\n",
    "                break\n",
    "            seqx, seqy = X[i:end], Y[end:outi]\n",
    "            Xs.append(seqx)\n",
    "            Ys.append(seqy)\n",
    "        return np.array(Xs), np.array(Ys)\n",
    "    \n",
    "    # Add time features\n",
    "    time_related_features = data[['Timestamp']].copy()      \n",
    "    time_related_features['hour_sin'] = np.sin(2 * np.pi * time_related_features['Timestamp'].dt.hour / 24)\n",
    "    time_related_features['hour_cos'] = np.cos(2 * np.pi * time_related_features['Timestamp'].dt.hour / 24)\n",
    "    time_related_features['day_of_week_sin'] = np.sin(2 * np.pi * time_related_features['Timestamp'].dt.dayofweek / 7)\n",
    "    time_related_features['day_of_week_cos'] = np.cos(2 * np.pi * time_related_features['Timestamp'].dt.dayofweek / 7)\n",
    "\n",
    "    # Categorical features\n",
    "    data.set_index('Timestamp', inplace=True)\n",
    "    data['dow'] = data.index.day_name().str[:3]     # e.g. \"Mon\",\"Tue\",…,\"Sun\"\n",
    "    data['hour'] = data.index.hour # 0–23\n",
    "    sun_low = data['dow'] == 'Sun' # basically all day on sunday\n",
    "    mon_low = (data['dow'] == 'Mon') & data['hour'].isin([0,1,2,3,4,5,6,7,8,9]) # basically from 0 to 9 on monday\n",
    "    wed_low = (data['dow'] == 'Wed') & data['hour'].isin([6,7,8,9,10,11,12,13,14]) # basically from 6 to 14 for maintenance on wednesday\n",
    "    sat_low = (data['dow'] == 'Sat') & data['hour'].isin([19,20,21,22,23]) # basically from 19 to 23 on saturday\n",
    "    data['is_low_usage'] = sun_low | mon_low | wed_low | sat_low\n",
    "    data.drop(columns=['dow', 'hour'], inplace=True)\n",
    "    data.reset_index(inplace=True)\n",
    "    data['is_low_usage'] = data['is_low_usage'].astype(int)\n",
    "    data['is_low_usage_next'] = data['is_low_usage'].shift(-1)\n",
    "    \n",
    "    X, Y, X_time= data.drop(columns=['Timestamp']), data['Energy'].values, time_related_features.drop(columns=['Timestamp'])\n",
    "    X = pd.concat([X, X_time], axis=1)\n",
    "    timestamps = data['Timestamp']\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X.values, Y, test_size=0.20, random_state=seed_val, shuffle=False)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.5, random_state=seed_val, shuffle=False)\n",
    "    # Split validation data 5% and 5% in order to test in the start and then use the other 5% for the second phase after selection\n",
    "    X_val1, X_val2, Y_val1, Y_val2 = train_test_split(X_val, Y_val, test_size=0.5, random_state=seed_val, shuffle=False)\n",
    "\n",
    "    # Create aligned timestamp splits\n",
    "    timestamps_train, timestamps_val, _Y_train_dummy, _Y_val_dummy = train_test_split(\n",
    "        timestamps, Y, test_size=0.20, random_state=seed_val, shuffle=False\n",
    "    )\n",
    "    timestamps_val, timestamps_test, _Y_val_dummy, _Y_test_dummy = train_test_split(\n",
    "        timestamps_val, _Y_val_dummy, test_size=0.5, random_state=seed_val, shuffle=False\n",
    "    )\n",
    "    timestamps_val1, timestamps_val2, _Y_val1_dummy, _Y_val2_dummy = train_test_split(\n",
    "        timestamps_val, _Y_val_dummy, test_size=0.5, random_state=seed_val, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # print(X_train.shape, type(X_train), X_val1.shape, type(X_val1))\n",
    "    X_train, Y_train = split_sequence(X_train, Y_train, n_past, n_future)\n",
    "    # X_val1, Y_val1 = split_sequence(X_val1, Y_val1, n_past, n_future)\n",
    "    X_test, Y_test = split_sequence(X_test, Y_test, n_past, n_future)\n",
    "    X_val1, Y_val1 = split_sequence(X_val1, Y_val1, n_past, n_future)\n",
    "    X_val2, Y_val2 = split_sequence(X_val2, Y_val2, n_past, n_future)\n",
    "\n",
    "  \n",
    "    # train-test split\n",
    "    # X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=seed_val, shuffle=False)\n",
    "    # X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.75, random_state=seed_val, shuffle=False)\n",
    "\n",
    "    #-----------Now the data is Train: 80%, Val_1: 5%, Val_2: 5%, Test: 10%----------------#\n",
    "\n",
    "    if testing_stage:\n",
    "        X_train = np.concatenate([X_train, X_val1], axis=0) # concatenate validation set to training set\n",
    "        Y_train = np.concatenate([Y_train, Y_val1], axis=0)\n",
    "        timestamps = timestamps_test  # concatenate timestamps for training set\n",
    "        X_val = X_val2  # use the second validation set for validation\n",
    "        Y_val = Y_val2  # use the second validation set for validation\n",
    "    else:\n",
    "        X_val = X_val1\n",
    "        Y_val = Y_val1\n",
    "        timestamps = timestamps_val1\n",
    "\n",
    "    # separate the lagged values, numerical and time features, time features are the last 4 features in the X, lagged value features is the last before the time features\n",
    "    X_numerical_train = X_train[:, :, :-7]    # all except last 7 (numerical features)\n",
    "    X_lagged_train    = X_train[:, :, -7:-6]  # just the 7th-from-last (lagged feature)\n",
    "    X_cat_train       = X_train[:, :, -6:-4]  # just the 6th-from-last (categorical feature)\n",
    "    X_time_train      = X_train[:, :, -4:]    # last 4 are time features (time-related features)\n",
    "\n",
    "    # VAL\n",
    "    X_numerical_val = X_val[:, :, :-7]\n",
    "    X_lagged_val    = X_val[:, :, -7:-6]\n",
    "    X_cat_val       = X_val[:, :, -6:-4]\n",
    "    X_time_val      = X_val[:, :, -4:]\n",
    "\n",
    "    # TEST\n",
    "    X_numerical_test = X_test[:, :, :-7]\n",
    "    X_lagged_test    = X_test[:, :, -7:-6]\n",
    "    X_cat_test       = X_test[:, :, -6:-4]\n",
    "    X_time_test      = X_test[:, :, -4:]\n",
    "\n",
    "\n",
    "    # broadcast ONLY is_low_usage_next across timesteps, in order to use it as a feature (low_usage from t+1 timestep) and because keep a constant value through the sequence\n",
    "    # TRAIN\n",
    "    X_cat_train[:, :, 1] = X_cat_train[:, -1, 1][:, None]\n",
    "    # VAL\n",
    "    X_cat_val[:,   :, 1] = X_cat_val[:,   -1, 1][:, None]\n",
    "    # TEST\n",
    "    X_cat_test[:,  :, 1] = X_cat_test[:,  -1, 1][:, None]\n",
    "\n",
    "\n",
    "    if data_augmentation:\n",
    "        fake_data = np.load(\"ddpm_fake_energy_raw.npy\")\n",
    "        fake_data = fake_data[:fake_data_length]\n",
    "        _, seq_len, F = fake_data.shape\n",
    "        X_fake = fake_data[:, :seq_len-1, :]\n",
    "        Y_fake = fake_data[:, seq_len-1, F-1]\n",
    "        Y_fake = Y_fake.reshape(-1, 1)\n",
    "        X_fake_numerical_train = X_fake[:, :, :-7] \n",
    "        X_fake_cat_train= X_fake[:, :, -7:-5] \n",
    "        X_fake_lagged_train = X_fake[:, :, -1:] \n",
    "        X_fake_time_train = X_fake[:, :, -5:-1] \n",
    "\n",
    "    if scaler_name == 'min-max':\n",
    "\n",
    "        N, seq_len, F = X_numerical_train.shape\n",
    "        X_numerical_train, scaler = min_max_scaler(X_numerical_train.reshape(-1, X_numerical_train.shape[-1]))\n",
    "        X_numerical_train = X_numerical_train.reshape(N, seq_len, F)\n",
    "        X_numerical_val = scaler.transform(X_numerical_val.reshape(-1, X_numerical_val.shape[-1]))\n",
    "        X_numerical_val = X_numerical_val.reshape(X_val.shape[0], seq_len, F)\n",
    "        X_numerical_test = scaler.transform(X_numerical_test.reshape(-1, X_numerical_test.shape[-1]))\n",
    "        X_numerical_test = X_numerical_test.reshape(X_test.shape[0], seq_len, F)\n",
    "            \n",
    "        Y_train, scaler_target = min_max_scaler(Y_train)\n",
    "        Y_val = scaler_target.transform(Y_val)\n",
    "        Y_test = scaler_target.transform(Y_test)\n",
    "        X_lagged_train = scaler_target.transform(X_lagged_train.reshape(-1, X_lagged_train.shape[-1]))\n",
    "        X_lagged_train = X_lagged_train.reshape(N, seq_len, 1)\n",
    "        X_lagged_val = scaler_target.transform(X_lagged_val.reshape(-1, X_lagged_val.shape[-1]))\n",
    "        X_lagged_val = X_lagged_val.reshape(X_val.shape[0], seq_len, 1)\n",
    "        X_lagged_test = scaler_target.transform(X_lagged_test.reshape(-1, X_lagged_test.shape[-1]))\n",
    "        X_lagged_test = X_lagged_test.reshape(X_test.shape[0], seq_len, 1)\n",
    "\n",
    "\n",
    "        #fake data augmentation\n",
    "        if data_augmentation:\n",
    "            N, seq_len, F = X_fake_numerical_train.shape\n",
    "\n",
    "            X_fake_numerical_train = scaler.transform(X_fake_numerical_train.reshape(-1, X_fake_numerical_train.shape[-1]))\n",
    "            X_fake_numerical_train = X_fake_numerical_train.reshape(N, seq_len, F)\n",
    "            X_fake_lagged_train = scaler_target.transform(X_fake_lagged_train.reshape(-1, X_fake_lagged_train.shape[-1]))\n",
    "            X_fake_lagged_train = X_fake_lagged_train.reshape(N, seq_len, 1)\n",
    "            Y_fake = scaler_target.transform(Y_fake)\n",
    "\n",
    "    elif scaler_name == 'standard':\n",
    "\n",
    "        N, seq_len, F = X_numerical_train.shape\n",
    "        X_numerical_train, scaler = standard_scaler(X_numerical_train.reshape(-1, X_numerical_train.shape[-1]))\n",
    "        X_numerical_train = X_numerical_train.reshape(N, seq_len, F)\n",
    "        X_numerical_val = scaler.transform(X_numerical_val.reshape(-1, X_numerical_val.shape[-1]))\n",
    "        X_numerical_val = X_numerical_val.reshape(X_val.shape[0], seq_len, F)\n",
    "        X_numerical_test = scaler.transform(X_numerical_test.reshape(-1, X_numerical_test.shape[-1]))\n",
    "        X_numerical_test = X_numerical_test.reshape(X_test.shape[0], seq_len, F)\n",
    "\n",
    "        Y_train, scaler_target = standard_scaler(Y_train)\n",
    "        Y_val = scaler_target.transform(Y_val)\n",
    "        Y_test = scaler_target.transform(Y_test)\n",
    "        X_lagged_train = scaler_target.transform(X_lagged_train.reshape(-1, X_lagged_train.shape[-1]))\n",
    "        X_lagged_train = X_lagged_train.reshape(N, seq_len, 1)\n",
    "        X_lagged_val = scaler_target.transform(X_lagged_val.reshape(-1, X_lagged_val.shape[-1]))\n",
    "        X_lagged_val = X_lagged_val.reshape(X_val.shape[0], seq_len, 1)\n",
    "        X_lagged_test = scaler_target.transform(X_lagged_test.reshape(-1, X_lagged_test.shape[-1]))\n",
    "        X_lagged_test = X_lagged_test.reshape(X_test.shape[0], seq_len, 1)\n",
    "\n",
    "\n",
    "        #fake data augmentation\n",
    "        if data_augmentation:\n",
    "            N, seq_len, F = X_fake_numerical_train.shape\n",
    "\n",
    "            X_fake_numerical_train = scaler.transform(X_fake_numerical_train.reshape(-1, X_fake_numerical_train.shape[-1]))\n",
    "            X_fake_numerical_train = X_fake_numerical_train.reshape(N, seq_len, F)\n",
    "            X_fake_lagged_train = scaler_target.transform(X_fake_lagged_train.reshape(-1, X_fake_lagged_train.shape[-1]))\n",
    "            X_fake_lagged_train = X_fake_lagged_train.reshape(N, seq_len, 1)\n",
    "            Y_fake = scaler_target.transform(Y_fake)\n",
    "\n",
    "    X_train = np.concatenate([X_numerical_train, X_cat_train, X_time_train, X_lagged_train], axis=2)    \n",
    "    X_val = np.concatenate([X_numerical_val, X_cat_val, X_time_val, X_lagged_val], axis=2)\n",
    "    X_test = np.concatenate([X_numerical_test, X_cat_test, X_time_test, X_lagged_test], axis=2)\n",
    "    # X_train = X_numerical_train\n",
    "    # X_val = X_numerical_val\n",
    "    # X_test = X_numerical_test\n",
    "\n",
    "    if data_augmentation:\n",
    "        X_fake = np.concatenate([X_fake_numerical_train, X_fake_cat_train, X_fake_time_train, X_fake_lagged_train], axis=2)\n",
    "        X_train, Y_train = np.concatenate([X_train, X_fake], axis=0), np.concatenate([Y_train, Y_fake], axis=0)\n",
    "\n",
    "\n",
    "    print(\"Size of training data is: \", X_train.shape)\n",
    "    quantiles = [0.05, 0.5, 0.95]\n",
    "    models = {}\n",
    "    histories = {}\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "                monitor='val_loss',     # or 'val_mae' depending on preference\n",
    "                patience=15,             # stop after 15 epochs with no improvement\n",
    "                restore_best_weights=True # rollback to the best model weights\n",
    "    )\n",
    "    if pinball_loss_usage:\n",
    "        for q in quantiles:\n",
    "            print(f\"Training {model_type} for quantile {q}...\")\n",
    "\n",
    "            if model_type == 'BiLSTM':\n",
    "                model = BiLSTM_model(X_train, q, pinball_loss_usage)\n",
    "            elif model_type == 'LSTM':\n",
    "                model = LSTM_model(X_train, q, pinball_loss_usage)\n",
    "\n",
    "            history = model.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=500, batch_size=batch_size, verbose=1, shuffle=True, callbacks=[early_stop, WandbMetricsLogger()])\n",
    "            models[q] = model\n",
    "            histories[q] = history\n",
    "    else:\n",
    "        print(f\"Training {model_type}...\")\n",
    "\n",
    "        if model_type == 'BiLSTM':\n",
    "            model = BiLSTM_model(X_train, 0, pinball_loss_usage)\n",
    "        elif model_type == 'LSTM':\n",
    "            model = LSTM_model(X_train, 0, pinball_loss_usage)\n",
    "        \n",
    "        model.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=500, batch_size=batch_size, verbose=1, shuffle=True, callbacks=[early_stop, WandbMetricsLogger()])\n",
    "        \n",
    "    # print(X_val.shape)\n",
    "    predictions = {}\n",
    "    if pinball_loss_usage:\n",
    "        if testing_stage:\n",
    "            for q in quantiles:\n",
    "                predictions[q] = models[q].predict(X_test)\n",
    "        else:\n",
    "            for q in quantiles:\n",
    "                predictions[q] = models[q].predict(X_val)\n",
    "    else:\n",
    "        if testing_stage:\n",
    "            predictions = model.predict(X_test)\n",
    "        else:\n",
    "            predictions = model.predict(X_val)\n",
    "\n",
    "    # print(X_val.shape)\n",
    "\n",
    "    if pinball_loss_usage:\n",
    "        y_pred_q05 = predictions[0.05]\n",
    "        y_pred_q50 = predictions[0.5]\n",
    "        y_pred_q95 = predictions[0.95]\n",
    "\n",
    "        predictions = scaler_target.inverse_transform(y_pred_q50.reshape(-1, 1))\n",
    "        y_pred_q05 = scaler_target.inverse_transform(y_pred_q05.reshape(-1, 1))\n",
    "        y_pred_q95 = scaler_target.inverse_transform(y_pred_q95.reshape(-1, 1))\n",
    "    else:\n",
    "        predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1))\n",
    "        \n",
    "    # predictions = model.predict(X_val)\n",
    "    if not testing_stage:\n",
    "        Y_test = Y_val\n",
    "\n",
    "    Y_test = scaler_target.inverse_transform(Y_test)\n",
    "\n",
    "\n",
    "    # Save predictions to a local file\n",
    "    filename = f\"model_name_{model_type}_predictions_seed_{seed_val}_augmentation_{batch_size}_scaler_{scaler_name}_DA_{data_augmentation}_pinball_{pinball_loss_usage}.npz\"\n",
    "    np.savez(filename, predictions=predictions.flatten(), ground_truth=Y_test.flatten())\n",
    "\n",
    "    # Create an artifact\n",
    "    artifact = wandb.Artifact(\n",
    "        name=f\"model_name_{model_type}_predictions_seed_{seed_val}_augmentation_{batch_size}_scaler_{scaler_name}_DA_{data_augmentation}_pinball_{pinball_loss_usage}\",\n",
    "        type=\"predictions\",\n",
    "        description=\"Predictions and ground truth for one run\",\n",
    "        metadata={\n",
    "            \"seed\": seed_val,\n",
    "            \"model\": model_type,\n",
    "            \"scaler\": scaler_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"data_aug\": data_augmentation,\n",
    "            \"fake_data_len\": fake_data_length\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add file and log it\n",
    "    artifact.add_file(filename)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    # Optional: Clean up local file\n",
    "    os.remove(filename)\n",
    "    \n",
    "\n",
    "\n",
    "    #------------QUANTILE METRICS-----------------#\n",
    "    if pinball_loss_usage:\n",
    "    # Pinball loss for quantiles\n",
    "        def pinball_loss(y_true, y_pred, q):\n",
    "            \"\"\"\n",
    "            Pinball loss for quantile q.\n",
    "            y_true, y_pred must be arrays of same shape.\n",
    "            \"\"\"\n",
    "            e = y_true - y_pred\n",
    "            return np.mean(np.maximum(q*e, (q-1)*e))\n",
    "        \n",
    "        loss_q05 = pinball_loss(Y_test, y_pred_q05, 0.05)\n",
    "        loss_q50 = pinball_loss(Y_test, y_pred_q50, 0.5)\n",
    "        loss_q95 = pinball_loss(Y_test, y_pred_q95, 0.95)\n",
    "\n",
    "        # Coverage (Calibration of prediction intervals)\n",
    "        def interval_coverage(y_true, y_lower, y_upper, nominal=0.90):\n",
    "            \"\"\"\n",
    "            Computes empirical coverage of [y_lower, y_upper].\n",
    "            \"\"\"\n",
    "            inside = (y_true >= y_lower) & (y_true <= y_upper)\n",
    "            empirical = np.mean(inside)\n",
    "            return empirical, empirical - nominal\n",
    "        \n",
    "        coverage_90, error_90 = interval_coverage(Y_test, y_pred_q05, y_pred_q95, nominal=0.90)\n",
    "\n",
    "\n",
    "        # Interval Width (Sharpness)\n",
    "        def interval_width(y_lower, y_upper):\n",
    "            return np.mean(y_upper - y_lower)\n",
    "        \n",
    "\n",
    "        sharpness_90 = interval_width(y_pred_q05, y_pred_q95)\n",
    "\n",
    "\n",
    "        wandb.log({\n",
    "        \"Pinball_0.05\": loss_q05,\n",
    "        \"Pinball_0.50\": loss_q50,\n",
    "        \"Pinball_0.95\": loss_q95,\n",
    "        \"Coverage_0.90\": coverage_90,\n",
    "        \"Coverage_Error_0.90\": error_90,\n",
    "        \"Sharpness_0.90\": sharpness_90\n",
    "        })\n",
    "\n",
    "    #------------QUANTILE METRICS END-----------------#\n",
    "\n",
    "    #------------Point forecast metrics-----------------#\n",
    "    mae = mean_absolute_error(Y_test, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, predictions))\n",
    "    r2 = r2_score(Y_test, predictions)\n",
    "    n = len(Y_test)\n",
    "    # p = shape2\n",
    "    # MASE function\n",
    "    def mase(y_true, y_pred, y_naive):\n",
    "        # y_true: Actual values\n",
    "        # y_pred: Predicted values\n",
    "        # y_naive: Naive model predictions (e.g., previous period's values)\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        y_naive = np.asarray(y_naive)\n",
    "        # Compute errors\n",
    "        errors = np.abs(y_true - y_pred)\n",
    "        \n",
    "        # Calculate MAE of naive model\n",
    "        naive_errors = np.abs(y_true[1:] - y_naive[1:])\n",
    "        mae_naive = np.mean(naive_errors)\n",
    "        \n",
    "        # Calculate MASE\n",
    "        mase_value = np.mean(errors) / mae_naive\n",
    "        return mase_value\n",
    "    # SMAPE function\n",
    "    def smape(y_true, y_pred):\n",
    "        # y_true: Actual values\n",
    "        # y_pred: Predicted values\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        # Calculate SMAPE\n",
    "        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "        diff = np.abs(y_true - y_pred) / denominator\n",
    "        smape_value = 100 * np.mean(diff)\n",
    "        return smape_value\n",
    "    smape_val = smape(Y_test, predictions)\n",
    "    y_naive = np.roll(Y_test, 1)\n",
    "    mase_val = mase(Y_test, predictions, y_naive)\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    print(f'Mean squared error (MSE): {mean_squared_error(Y_test, predictions)}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "    print(f'R² Score: {r2}')\n",
    "    print(f\"SMAPE score: {smape_val}\")\n",
    "    print(f\"MASE score: {mase_val}\")\n",
    "\n",
    "    print(predictions.shape, Y_test.shape)\n",
    "    if pinball_loss_usage:\n",
    "        # pinball loss plot\n",
    "        timestamps = timestamps[48:-1]\n",
    "        print(timestamps.shape)\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(timestamps, Y_test.squeeze(), label=\"Ground Truth\", color='black', linewidth=2)\n",
    "        plt.plot(timestamps, predictions.squeeze(), label=\"Median Prediction (0.5)\", color='#0072B2', linewidth=2)\n",
    "        plt.fill_between(\n",
    "            timestamps, y_pred_q05.squeeze(), y_pred_q95.squeeze(),\n",
    "            color='#0072B2', alpha=0.2, label=\"90% Confidence Interval (0.05–0.95)\"\n",
    "        )\n",
    "\n",
    "        # Custom x-axis formatter → weekday + month-day + hour:00\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%a %m-%d %H:%M'))\n",
    "\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.title(f\"{model_type} Quantile Regression - Confidence Interval\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Energy\")\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"train vs val_predictions\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "    else: # mse loss plot\n",
    "        timestamps = timestamps[48:-1]\n",
    "        print(timestamps.shape)\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(timestamps, Y_test.squeeze(), label=\"Ground Truth\", color='black', linewidth=2)\n",
    "        plt.plot(timestamps, predictions.squeeze(), label=\"Prediction\", color='#0072B2', linewidth=2)\n",
    "\n",
    "        # Custom x-axis formatter → weekday + month-day + hour:00\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%a %m-%d %H:%M'))\n",
    "\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.title(f\"{model_type} - MSE Regression\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Energy\")\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"train vs val_predictions\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "\n",
    "    wandb.log({\n",
    "    \"val_MAE\": mae,\n",
    "    \"val_MSE\": mean_squared_error(Y_test, predictions),\n",
    "    \"val_RMSE\": rmse,\n",
    "    \"val_R2\": r2,\n",
    "    \"val_SMAPE\": smape_val,\n",
    "    \"val_MASE\": mase_val\n",
    "    })\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'model_name': {\n",
    "            'values': ['BiLSTM']\n",
    "            # 'values': ['BiLSTM', 'LSTM']\n",
    "        },\n",
    "        'scaler_name': {\n",
    "            'values': ['standard']\n",
    "            \n",
    "            # 'values': ['standard', 'min-max']\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 16]\n",
    "            # 'values': [32]\n",
    "        },\n",
    "        'data_augmentation': {\n",
    "            'values': [True]\n",
    "        },\n",
    "        'seed': {\n",
    "            'values': [42, 4242, 777, 2021, 1234]\n",
    "            # 'values' : [2021, 1234, 777]\n",
    "        },\n",
    "        'fake_data_length': {\n",
    "            # 'values': [200, 400, 800, 1200, 1600]\n",
    "            'values': [200, 400, 800, 1200, 1600]\n",
    "        },\n",
    "        'pinball_loss_usage': {\n",
    "            'values': [False]\n",
    "        },\n",
    "        'testing_stage': {\n",
    "            'values': [True]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "project_base = f\"HALCOR-ENERGY-TIMESERIES-FORECASTING-MSE-TESTING\"\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project=project_base)\n",
    "\n",
    "\n",
    "def sweep_train():\n",
    "    run = wandb.init()\n",
    "\n",
    "    # Extract config values\n",
    "    model = run.config.model_name\n",
    "    scaler = run.config.scaler_name\n",
    "    batch_size = run.config.batch_size\n",
    "\n",
    "    run.name = f\"{model}-{scaler}-bs{batch_size}-aug{run.config.data_augmentation}--seed{run.config.seed}--fake_data_length{run.config.fake_data_length}---pinball_loss_usage{run.config.pinball_loss_usage}---testing_stage{run.config.testing_stage}\"\n",
    "\n",
    "    # Run your training pipeline\n",
    "    training(\n",
    "        seed_val=run.config.seed,\n",
    "        model_type=model,\n",
    "        batch_size=batch_size,\n",
    "        scaler_name=scaler,\n",
    "        data_augmentation=run.config.data_augmentation,\n",
    "        fake_data_length=run.config.fake_data_length,\n",
    "        testing_stage=run.config.testing_stage,\n",
    "        pinball_loss_usage=run.config.pinball_loss_usage\n",
    "    )\n",
    "\n",
    "wandb.agent(sweep_id, function=sweep_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597ed58",
   "metadata": {},
   "source": [
    "# XGBoost pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_training(model_name = 'XGBoost', testing_stage = False, quantile_usage = False, seed_val = 42):\n",
    "    # Define seed for reproducibility\n",
    "    \n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_val)\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    tf.random.set_seed(seed_val)\n",
    "\n",
    "    data = ori_data.copy()\n",
    "\n",
    "    # Preprocessing\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "    sensor_feats =  data.drop(columns=['Timestamp', 'Energy'])\n",
    "    # ---------- 1) Target: next-step energy ----------\n",
    "    data['lag_0'] = data['Energy']          # Energy at t (current)\n",
    "    data['y_next'] = data['Energy'].shift(-1)\n",
    "\n",
    "    # ---------- 2) Lags: past-only info ----------\n",
    "    for k in range(1, 8):\n",
    "        data[f'lag_{k}'] = data['Energy'].shift(k)\n",
    "\n",
    "    # ---------- 3) Time features for the *prediction time* (t+1) ----------\n",
    "    ts_next = data['Timestamp'].shift(-1)  # the moment we're predicting for\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * ts_next.dt.hour / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * ts_next.dt.hour / 24)\n",
    "    data['dow_sin']  = np.sin(2 * np.pi * ts_next.dt.dayofweek / 7)\n",
    "    data['dow_cos']  = np.cos(2 * np.pi * ts_next.dt.dayofweek / 7)\n",
    "\n",
    "    # ---------- 4) Categorical \"low usage\" flag at prediction time (t+1) ----------\n",
    "    dow3  = ts_next.dt.day_name().str[:3]\n",
    "    hour  = ts_next.dt.hour\n",
    "    sun_low = dow3 == 'Sun'\n",
    "    mon_low = (dow3 == 'Mon') & hour.isin([0,1,2,3,4,5,6,7,8,9])\n",
    "    wed_low = (dow3 == 'Wed') & hour.isin([6,7,8,9,10,11,12,13,14])\n",
    "    sat_low = (dow3 == 'Sat') & hour.isin([19,20,21,22,23])\n",
    "    data['is_low_usage'] = (sun_low | mon_low | wed_low | sat_low).astype(int)\n",
    "    data['is_low_usage_next'] = data['is_low_usage'].shift(-1)  # next step low usage flag\n",
    "\n",
    "    # ---------- 5) Build the modeling table and keep rows aligned ----------\n",
    "    sensor_cols = sensor_feats.columns.tolist()\n",
    "    feature_cols = (\n",
    "        [f'lag_{k}' for k in range(0, 8)] +           # Energy lag_0..lag_7\n",
    "        sensor_cols +                                 # all current sensor readings (t)\n",
    "        ['is_low_usage', 'is_low_usage_next', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
    "    )\n",
    "\n",
    "    df_model = data[['Timestamp', 'y_next'] + feature_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "    # Final X, y (names kept for clarity)\n",
    "    X = df_model[feature_cols].to_numpy()\n",
    "    Y = df_model['y_next'].to_numpy().reshape(-1, 1)\n",
    "    timestamps = df_model['Timestamp'] # optional for plotting later\n",
    "   \n",
    "    column_names = list(df_model[feature_cols].columns)\n",
    "    # X_concat = pd.concat([X_num, X_time], axis=1)\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.20, random_state=seed_val, shuffle=False)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.5, random_state=seed_val, shuffle=False)\n",
    "    # Split validation data 5% and 5% in order to test in the start and then use the other 5% for the second phase after selection\n",
    "    X_val1, X_val2, Y_val1, Y_val2 = train_test_split(X_val, Y_val, test_size=0.5, random_state=seed_val, shuffle=False)\n",
    "\n",
    "    print(Y.shape, X.shape, timestamps.shape)\n",
    "    # Create aligned timestamp splits\n",
    "    timestamps_train, timestamps_val, _Y_train_dummy, _Y_val_dummy = train_test_split(\n",
    "        timestamps, Y, test_size=0.20, random_state=seed_val, shuffle=False\n",
    "    )\n",
    "    timestamps_val, timestamps_test, _Y_val_dummy, _Y_test_dummy = train_test_split(\n",
    "        timestamps_val, _Y_val_dummy, test_size=0.5, random_state=seed_val, shuffle=False\n",
    "    )\n",
    "    timestamps_val1, timestamps_val2, _Y_val1_dummy, _Y_val2_dummy = train_test_split(\n",
    "        timestamps_val, _Y_val_dummy, test_size=0.5, random_state=seed_val, shuffle=False\n",
    "    )\n",
    "\n",
    "    if testing_stage:\n",
    "\n",
    "        X_train = np.concatenate([X_train, X_val1], axis=0)\n",
    "        Y_train = np.concatenate([Y_train, Y_val1], axis=0)\n",
    "        timestamps = timestamps_test\n",
    "        X_val = X_val2  # use the second validation set for validation\n",
    "        Y_val = Y_val2\n",
    "    else:\n",
    "\n",
    "        X_val = X_val1\n",
    "        Y_val = Y_val1\n",
    "        timestamps = timestamps_val1\n",
    "\n",
    "    Y_train = Y_train.reshape(-1, 1)\n",
    "    Y_val = Y_val.reshape(-1, 1)\n",
    "    Y_test = Y_test.reshape(-1, 1)\n",
    "    # separate the lagged values, numerical and time features, time features are the last 4 features in the X, lagged value features is the last before the time features\n",
    "    \n",
    "    n_lags = 8  # number of lagged features\n",
    "    n_sensors = len(sensor_cols)  # number of sensor features\n",
    "    # TRAIN\n",
    "    X_lagged_train = X_train[:, :n_lags]  # lagged features\n",
    "    X_numerical_train = X_train[:, :n_lags + n_sensors]   # lags + sensors\n",
    "    X_cat_train       = X_train[:, n_lags + n_sensors : n_lags + n_sensors + 2]  # is_low_usage\n",
    "    X_time_train      = X_train[:, -4:]                   # last 4 time features\n",
    "\n",
    "    # VAL\n",
    "    X_lagged_val = X_val[:, :n_lags]\n",
    "    X_numerical_val = X_val[:, :n_lags + n_sensors]\n",
    "    X_cat_val       = X_val[:, n_lags + n_sensors : n_lags + n_sensors + 2]\n",
    "    X_time_val      = X_val[:, -4:]\n",
    "\n",
    "    # TEST\n",
    "    X_lagged_test = X_test[:, :n_lags]\n",
    "    X_numerical_test = X_test[:, :n_lags + n_sensors]\n",
    "    X_cat_test       = X_test[:, n_lags + n_sensors : n_lags + n_sensors + 2]\n",
    "    X_time_test      = X_test[:, -4:]\n",
    "\n",
    "    # print(X_lagged.shape, X_num.shape, X_time.shape)\n",
    "    # save the data to csv files\n",
    "\n",
    "    # Normalization based on the scaler choice\n",
    "\n",
    "    # Standard Scaler\n",
    "    X_numerical_train, scaler = standard_scaler(X_numerical_train)\n",
    "    X_numerical_val = scaler.transform(X_numerical_val)\n",
    "    X_numerical_test = scaler.transform(X_numerical_test)\n",
    "    Y_train, scaler_target = standard_scaler(Y_train)  # Reshape to ensure it's a 2D array\n",
    "    \n",
    "    # manually scale the lagged features\n",
    "    lag_mean = scaler_target.mean_[0]\n",
    "    lag_std  = scaler_target.scale_[0]\n",
    "    X_lagged_train = (X_lagged_train - lag_mean) / lag_std\n",
    "    X_lagged_val   = (X_lagged_val - lag_mean) / lag_std\n",
    "    X_lagged_test  = (X_lagged_test - lag_mean) / lag_std\n",
    "    Y_val = scaler_target.transform(Y_val)\n",
    "    Y_test = scaler_target.transform(Y_test)\n",
    "\n",
    "    X_train = np.concatenate([X_numerical_train, X_cat_train, X_time_train, X_lagged_train], axis=1)    \n",
    "    X_val = np.concatenate([X_numerical_val, X_cat_val, X_time_val, X_lagged_val], axis=1)\n",
    "    X_test = np.concatenate([X_numerical_test, X_cat_test, X_time_test, X_lagged_test], axis=1)\n",
    "\n",
    "    if quantile_usage:\n",
    "        if testing_stage:\n",
    "            results = XGBoost_model(X_train, Y_train, X_val, Y_val, X_test, Y_test, seed_val, True)\n",
    "        else:\n",
    "            results = XGBoost_model(X_train, Y_train, X_val, Y_val, X_val, Y_val, seed_val, True)\n",
    "    else:\n",
    "        if testing_stage:\n",
    "            results, model = XGBoost_model(X_train, Y_train, X_val, Y_val, X_test, Y_test, seed_val, False)\n",
    "            Y_test = Y_test[48:-1]\n",
    "        else:\n",
    "            results, model = XGBoost_model(X_train, Y_train, X_val, Y_val, X_val, Y_val, seed_val, False)\n",
    "            Y_val = Y_val[48:-1]\n",
    "            Y_test = Y_val\n",
    "            print(Y_test.shape)\n",
    "\n",
    "    if quantile_usage:\n",
    "        model = results['models'][0.5]  # Get the model for the median quantile\n",
    "        Y_test = results['y_true']\n",
    "        predictions = results['predictions'][0.5]\n",
    "        lower = results['predictions'][0.05]\n",
    "        upper = results['predictions'][0.95]\n",
    "        lower = scaler_target.inverse_transform(lower.reshape(-1, 1))\n",
    "        upper = scaler_target.inverse_transform(upper.reshape(-1, 1))\n",
    "        \n",
    "    else:\n",
    "        predictions = results\n",
    "    # Just so we can return the same format as the other models\n",
    "\n",
    "    # predictions = model.predict(X_val)\n",
    "    # Y_test = Y_val\n",
    "    predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1))\n",
    "    \n",
    "    Y_test = scaler_target.inverse_transform(Y_test)\n",
    "    print(\"Shapes - Predictions: {}, Y_test: {}\".format(predictions.shape, Y_test.shape))\n",
    "    # model = results['models'][0.5]\n",
    "\n",
    "    # Save predictions to a local file\n",
    "    filename = f\"model_name_XGBoost_predictions_seed_{seed_val}_augmentation_{0}_scaler_standard_pinball_{quantile_usage}.npz\"\n",
    "    np.savez(filename, predictions=predictions.flatten(), ground_truth=Y_test.flatten())\n",
    "\n",
    "    # Create an artifact\n",
    "    artifact = wandb.Artifact(\n",
    "        name=f\"model_name_XGBoost_predictions_seed_{seed_val}_augmentation_{0}_scaler_standard_pinball_{quantile_usage}\",\n",
    "        type=\"predictions\",\n",
    "        description=\"Predictions and ground truth for one run\",\n",
    "        metadata={\n",
    "            \"seed\": seed_val,\n",
    "            \"model\": 'XGBoost',\n",
    "            \"scaler\": 'standard',\n",
    "            \"batch_size\": 0,\n",
    "            \"quantile\" : quantile_usage,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add file and log it\n",
    "    artifact.add_file(filename)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    # Optional: Clean up local file\n",
    "    os.remove(filename)\n",
    "\n",
    "    \n",
    "    #------------QUANTILE METRICS-----------------#\n",
    "    if quantile_usage:\n",
    "    # Pinball loss for quantiles\n",
    "        def pinball_loss(y_true, y_pred, q):\n",
    "            \"\"\"\n",
    "            Pinball loss for quantile q.\n",
    "            y_true, y_pred must be arrays of same shape.\n",
    "            \"\"\"\n",
    "            e = y_true - y_pred\n",
    "            return np.mean(np.maximum(q*e, (q-1)*e))\n",
    "        \n",
    "        loss_q05 = pinball_loss(Y_test, lower, 0.05)\n",
    "        loss_q50 = pinball_loss(Y_test, predictions, 0.5)\n",
    "        loss_q95 = pinball_loss(Y_test, upper, 0.95)\n",
    "\n",
    "        # Coverage (Calibration of prediction intervals)\n",
    "        def interval_coverage(y_true, y_lower, y_upper, nominal=0.90):\n",
    "            \"\"\"\n",
    "            Computes empirical coverage of [y_lower, y_upper].\n",
    "            \"\"\"\n",
    "            inside = (y_true >= y_lower) & (y_true <= y_upper)\n",
    "            empirical = np.mean(inside)\n",
    "            return empirical, empirical - nominal\n",
    "        \n",
    "        coverage_90, error_90 = interval_coverage(Y_test, lower, upper, nominal=0.90)\n",
    "\n",
    "\n",
    "        # Interval Width (Sharpness)\n",
    "        def interval_width(y_lower, y_upper):\n",
    "            return np.mean(y_upper - y_lower)\n",
    "\n",
    "        sharpness_90 = interval_width(lower, upper)\n",
    "\n",
    "        wandb.log({\n",
    "        \"Pinball_0.05\": loss_q05,\n",
    "        \"Pinball_0.50\": loss_q50,\n",
    "        \"Pinball_0.95\": loss_q95,\n",
    "        \"Coverage_0.90\": coverage_90,\n",
    "        \"Coverage_Error_0.90\": error_90,\n",
    "        \"Sharpness_0.90\": sharpness_90\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    #------------QUANTILE METRICS END-----------------#\n",
    "\n",
    "    #------------Point forecast metrics-----------------#\n",
    "    mae = mean_absolute_error(Y_test, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, predictions))\n",
    "    r2 = r2_score(Y_test, predictions)\n",
    "    n = len(Y_test)\n",
    "    # p = shape2\n",
    "    # MASE function\n",
    "    def mase(y_true, y_pred, y_naive):\n",
    "        # y_true: Actual values\n",
    "        # y_pred: Predicted values\n",
    "        # y_naive: Naive model predictions (e.g., previous period's values)\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        y_naive = np.asarray(y_naive)\n",
    "        # Compute errors\n",
    "        errors = np.abs(y_true - y_pred)\n",
    "        \n",
    "        # Calculate MAE of naive model\n",
    "        naive_errors = np.abs(y_true[1:] - y_naive[1:])\n",
    "        mae_naive = np.mean(naive_errors)\n",
    "        \n",
    "        # Calculate MASE\n",
    "        mase_value = np.mean(errors) / mae_naive\n",
    "        return mase_value\n",
    "    # SMAPE function\n",
    "    def smape(y_true, y_pred):\n",
    "        # y_true: Actual values\n",
    "        # y_pred: Predicted values\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        # Calculate SMAPE\n",
    "        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "        diff = np.abs(y_true - y_pred) / denominator\n",
    "        smape_value = 100 * np.mean(diff)\n",
    "        return smape_value\n",
    "    smape_val = smape(Y_test, predictions)\n",
    "    y_naive = np.roll(Y_test, 1)\n",
    "    mase_val = mase(Y_test, predictions, y_naive)\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    print(f'Mean squared error (MSE): {mean_squared_error(Y_test, predictions)}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "    print(f'R² Score: {r2}')\n",
    "    print(f\"SMAPE score: {smape_val}\")\n",
    "    print(f\"MASE score: {mase_val}\")\n",
    "\n",
    "    if quantile_usage:\n",
    "        # pinball loss plot\n",
    "        timestamps = timestamps[48:-1]\n",
    "        print(timestamps.shape)\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(timestamps, Y_test.squeeze(), label=\"Ground Truth\", color='black', linewidth=2)\n",
    "        plt.plot(timestamps, predictions.squeeze(), label=\"Median Prediction (0.5)\", color='#0072B2', linewidth=2)\n",
    "        plt.fill_between(\n",
    "            timestamps, lower.squeeze(), upper.squeeze(),\n",
    "            color='#0072B2', alpha=0.2, label=\"90% Confidence Interval (0.05–0.95)\"\n",
    "        )\n",
    "\n",
    "        # Custom x-axis formatter → weekday + month-day + hour:00\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%a %m-%d %H:%M'))\n",
    "\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.title(f\"{'XGBoost'} Quantile Regression - Confidence Interval\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Energy\")\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"train vs val_predictions\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "    else: # mse loss plot\n",
    "        print(timestamps.shape)\n",
    "        timestamps = timestamps[48:-1]\n",
    "        print(timestamps.shape)\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(timestamps, Y_test.squeeze(), label=\"Ground Truth\", color='black', linewidth=2)\n",
    "        plt.plot(timestamps, predictions.squeeze(), label=\"Prediction\", color='#0072B2', linewidth=2)\n",
    "\n",
    "        # Custom x-axis formatter → weekday + month-day + hour:00\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%a %m-%d %H:%M'))\n",
    "\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.title(f\"{'XGBoost'} - MSE Regression\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Energy\")\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"train vs val_predictions\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "\n",
    "    wandb.log({\n",
    "    \"val_MAE\": mae,\n",
    "    \"val_MSE\": mean_squared_error(Y_test, predictions),\n",
    "    \"val_RMSE\": rmse,\n",
    "    \"val_R2\": r2,\n",
    "    \"val_SMAPE\": smape_val,\n",
    "    \"val_MASE\": mase_val\n",
    "    })\n",
    "\n",
    "    \n",
    "    importance = model.get_score(importance_type='weight') # For plotting SHAP values later at prediction\n",
    "    # Convert the importance dictionary to a DataFrame for easier plotting\n",
    "    importance_df = pd.DataFrame(importance.items(), columns=['Feature', 'Importance'])\n",
    "    column_names = list(column_names)\n",
    "    importance_df['Feature'] = importance_df['Feature'].replace(\n",
    "            {f'f{i}': column_names[i] for i in range(len(column_names))}\n",
    "    )\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    top_features = importance_df[:30] if len(importance_df) > 30 else importance_df\n",
    "    plt.barh(top_features['Feature'], top_features['Importance'])\n",
    "\n",
    "    plt.xlabel('Importance')\n",
    "    plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to avoid cutting off labels\n",
    "    # plt.subplots_adjust(left=0.3)  # Uncomment if tight_layout isn't enough\n",
    "\n",
    "    wandb.log({\"Feature Importance\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'model_name': {\n",
    "            # 'values': ['XGBoost', 'RandomForest']\n",
    "            'values': ['XGBoost']\n",
    "        },\n",
    "        'scaler_name': {\n",
    "            'values': ['standard']\n",
    "            \n",
    "            # 'values': ['standard', 'min-max']\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [0]\n",
    "            # 'values': [128]\n",
    "        },\n",
    "        'data_augmentation': {\n",
    "            'values': [False]\n",
    "        },\n",
    "        'seed': {\n",
    "            # 'values': [777, 2021, 1234, 4242, 42]\n",
    "            'values' : [42]\n",
    "        },\n",
    "        'fake_data_length': {\n",
    "            # 'values': [200, 400, 1000, 1800]\n",
    "            'values': [0]\n",
    "        },\n",
    "        'pinball_loss_usage': {\n",
    "            'values': [True]\n",
    "        },\n",
    "        'testing_stage': {\n",
    "            'values': [False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "project_base = f\"HALCOR-ENERGY-TIMESERIES-FORECASTING-PINBALL-VALIDATION\"\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project=project_base)\n",
    "\n",
    "\n",
    "def sweep_train():\n",
    "    run = wandb.init()\n",
    "\n",
    "    # Extract config values\n",
    "    model = run.config.model_name\n",
    "    scaler = run.config.scaler_name\n",
    "    batch_size = run.config.batch_size\n",
    "\n",
    "    run.name = f\"{model}-{scaler}-bs{batch_size}-aug{run.config.data_augmentation}--seed{run.config.seed}--fake_data_length{run.config.fake_data_length}---pinball_loss_usage{run.config.pinball_loss_usage}---testing_stage{run.config.testing_stage}\"\n",
    "\n",
    "    # Run your training pipeline\n",
    "    xgb_training(model_name=model, testing_stage=run.config.testing_stage,\n",
    "                 quantile_usage=run.config.pinball_loss_usage,)\n",
    "\n",
    "wandb.agent(sweep_id, function=sweep_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b736e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ori_data.copy()\n",
    "seed_val = 42\n",
    "# Preprocessing\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "sensor_feats =  data.drop(columns=['Timestamp', 'Energy'])\n",
    "\n",
    "# ---------- 1) Target: next-step energy ----------\n",
    "data['lag_0'] = data['Energy']          # Energy at t (current)\n",
    "data['y_next'] = data['Energy'].shift(-1)\n",
    "\n",
    "# ---------- 2) Lags: past-only info ----------\n",
    "for k in range(1, 8):\n",
    "    data[f'lag_{k}'] = data['Energy'].shift(k)\n",
    "\n",
    "# ---------- 3) Time features for the *prediction time* (t+1) ----------\n",
    "ts_next = data['Timestamp'].shift(-1)  # the moment we're predicting for\n",
    "data['hour_sin'] = np.sin(2 * np.pi * ts_next.dt.hour / 24)\n",
    "data['hour_cos'] = np.cos(2 * np.pi * ts_next.dt.hour / 24)\n",
    "data['dow_sin']  = np.sin(2 * np.pi * ts_next.dt.dayofweek / 7)\n",
    "data['dow_cos']  = np.cos(2 * np.pi * ts_next.dt.dayofweek / 7)\n",
    "\n",
    "# ---------- 4) Categorical \"low usage\" flag at prediction time (t+1) ----------\n",
    "dow3  = ts_next.dt.day_name().str[:3]\n",
    "hour  = ts_next.dt.hour\n",
    "sun_low = dow3 == 'Sun'\n",
    "mon_low = (dow3 == 'Mon') & hour.isin([0,1,2,3,4,5,6,7,8,9])\n",
    "wed_low = (dow3 == 'Wed') & hour.isin([6,7,8,9,10,11,12,13,14])\n",
    "sat_low = (dow3 == 'Sat') & hour.isin([19,20,21,22,23])\n",
    "data['is_low_usage'] = (sun_low | mon_low | wed_low | sat_low).astype(int)\n",
    "\n",
    "\n",
    "# ---------- 5) Build the modeling table and keep rows aligned ----------\n",
    "sensor_cols = sensor_feats.columns.tolist()\n",
    "feature_cols = (\n",
    "    [f'lag_{k}' for k in range(0, 8)] +           # Energy lag_0..lag_7\n",
    "    sensor_cols +                                 # all current sensor readings (t)\n",
    "    ['is_low_usage', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
    ")\n",
    "\n",
    "df_model = data[['Timestamp', 'y_next'] + feature_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "X = df_model[feature_cols].to_numpy()\n",
    "Y = df_model['y_next'].to_numpy().reshape(-1, 1)\n",
    "timestamps = df_model['Timestamp']\n",
    "column_names = list(df_model[feature_cols].columns)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.20, random_state=seed_val, shuffle=False)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.5, random_state=seed_val, shuffle=False)\n",
    "# Split validation data 5% and 5% in order to test in the start and then use the other 5% for the second phase after selection\n",
    "X_val1, X_val2, Y_val1, Y_val2 = train_test_split(X_val, Y_val, test_size=0.5, random_state=seed_val, shuffle=False)\n",
    "\n",
    "\n",
    "# Create aligned timestamp splits\n",
    "timestamps_train, timestamps_val, _Y_train_dummy, _Y_val_dummy = train_test_split(\n",
    "    timestamps, Y, test_size=0.20, random_state=seed_val, shuffle=False\n",
    ")\n",
    "timestamps_val, timestamps_test, _Y_val_dummy, _Y_test_dummy = train_test_split(\n",
    "    timestamps_val, _Y_val_dummy, test_size=0.5, random_state=seed_val, shuffle=False\n",
    ")\n",
    "timestamps_val1, timestamps_val2, _Y_val1_dummy, _Y_val2_dummy = train_test_split(\n",
    "    timestamps_val, _Y_val_dummy, test_size=0.5, random_state=seed_val, shuffle=False\n",
    ")\n",
    "\n",
    "timestamps_train = pd.concat([timestamps_train, timestamps_val1])\n",
    "\n",
    "X_train = np.concatenate([X_train, X_val1], axis=0)\n",
    "Y_train = np.concatenate([Y_train, Y_val1], axis=0)\n",
    "\n",
    "# separate the lagged values, numerical and time features, time features are the last 4 features in the X, lagged value features is the last before the time features\n",
    "# TRAIN\n",
    "n_lags = 8  # number of lagged features\n",
    "n_sensors = len(sensor_cols)  # number of sensor features\n",
    "\n",
    "# TRAIN\n",
    "X_lagged_train = X_train[:, :n_lags]  # lagged features\n",
    "X_numerical_train = X_train[:, :n_lags + n_sensors]   # lags + sensors\n",
    "X_cat_train       = X_train[:, n_lags + n_sensors : n_lags + n_sensors + 1]  # is_low_usage\n",
    "X_time_train      = X_train[:, -4:]                   # last 4 time features\n",
    "\n",
    "# VAL\n",
    "X_lagged_val = X_val2[:, :n_lags]\n",
    "X_numerical_val = X_val2[:, :n_lags + n_sensors]\n",
    "X_cat_val       = X_val2[:, n_lags + n_sensors : n_lags + n_sensors + 1]\n",
    "X_time_val      = X_val2[:, -4:]\n",
    "\n",
    "# TEST\n",
    "X_lagged_test = X_test[:, :n_lags]\n",
    "X_numerical_test = X_test[:, :n_lags + n_sensors]\n",
    "X_cat_test       = X_test[:, n_lags + n_sensors : n_lags + n_sensors + 1]\n",
    "X_time_test      = X_test[:, -4:]\n",
    "\n",
    "\n",
    "# print(X_lagged.shape, X_num.shape, X_time.shape)\n",
    "# save the data to csv files\n",
    "\n",
    "# Normalization based on the scaler choice\n",
    "\n",
    "# Standard Scaler\n",
    "X_numerical_train, scaler = standard_scaler(X_numerical_train)\n",
    "X_numerical_val = scaler.transform(X_numerical_val)\n",
    "X_numerical_test = scaler.transform(X_numerical_test)\n",
    "Y_train, scaler_target = standard_scaler(Y_train)  # Reshape to ensure it's a 2D array\n",
    "\n",
    "Y_val = scaler_target.transform(Y_val2)\n",
    "Y_test = scaler_target.transform(Y_test)\n",
    "# manually scale the lagged features\n",
    "lag_mean = scaler_target.mean_[0]\n",
    "lag_std  = scaler_target.scale_[0]\n",
    "X_lagged_train = (X_lagged_train - lag_mean) / lag_std\n",
    "X_lagged_val   = (X_lagged_val - lag_mean) / lag_std\n",
    "X_lagged_test  = (X_lagged_test - lag_mean) / lag_std\n",
    "\n",
    "X_train = np.concatenate([X_numerical_train, X_cat_train, X_lagged_train, X_time_train], axis=1)    \n",
    "X_val = np.concatenate([X_numerical_val, X_cat_val, X_lagged_val, X_time_val], axis=1)\n",
    "X_test = np.concatenate([X_numerical_test, X_cat_test, X_lagged_test, X_time_test], axis=1)\n",
    "\n",
    "results = XGBoost_model(X_train, Y_train, X_val, Y_val, X_test, Y_test, seed_val)\n",
    "\n",
    "Y_test = results['y_true']\n",
    "predictions = results['predictions'][0.5]\n",
    "lower = results['predictions'][0.05]\n",
    "upper = results['predictions'][0.95]\n",
    "# Just so we can return the same format as the other models\n",
    "\n",
    "# predictions = model.predict(X_val)\n",
    "# Y_test = Y_val\n",
    "predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1))\n",
    "lower = scaler_target.inverse_transform(lower.reshape(-1, 1))\n",
    "upper = scaler_target.inverse_transform(upper.reshape(-1, 1))\n",
    "Y_test = scaler_target.inverse_transform(Y_test)\n",
    "\n",
    "model = results['models'][0.5]\n",
    "mae = mean_absolute_error(Y_test, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, predictions))\n",
    "r2 = r2_score(Y_test, predictions)\n",
    "n = len(Y_test)\n",
    "# p = shape2\n",
    "# MASE function\n",
    "def mase(y_true, y_pred, y_naive):\n",
    "    # y_true: Actual values\n",
    "    # y_pred: Predicted values\n",
    "    # y_naive: Naive model predictions (e.g., previous period's values)\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_naive = np.asarray(y_naive)\n",
    "    # Compute errors\n",
    "    errors = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Calculate MAE of naive model\n",
    "    naive_errors = np.abs(y_true[1:] - y_naive[1:])\n",
    "    mae_naive = np.mean(naive_errors)\n",
    "    \n",
    "    # Calculate MASE\n",
    "    mase_value = np.mean(errors) / mae_naive\n",
    "    return mase_value\n",
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    # y_true: Actual values\n",
    "    # y_pred: Predicted values\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    # Calculate SMAPE\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    smape_value = 100 * np.mean(diff)\n",
    "    return smape_value\n",
    "smape_val = smape(Y_test, predictions)\n",
    "y_naive = np.roll(Y_test, 1)\n",
    "mase_val = mase(Y_test, predictions, y_naive)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Mean squared error (MSE): {mean_squared_error(Y_test, predictions)}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'R² Score: {r2}')\n",
    "print(f\"SMAPE score: {smape_val}\")\n",
    "print(f\"MASE score: {mase_val}\")\n",
    "\n",
    "timestamps_test = timestamps_test[48:-1]\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Plot\n",
    "plt.plot(timestamps_test, Y_test.squeeze(), label=\"Ground Truth\", color='black', linewidth=2)\n",
    "plt.plot(timestamps_test, predictions.squeeze(), label=\"Median Prediction (0.5)\", color='#0072B2', linewidth=2)\n",
    "plt.fill_between(timestamps_test, lower.squeeze(), upper.squeeze(),\n",
    "                 color='#0072B2', alpha=0.2, label=\"90% Confidence Interval (0.05–0.95)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.title(\"XGBoost Quantile Regression - Confidence Interval\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "importance = model.get_score(importance_type='weight') # For plotting SHAP values later at prediction\n",
    "# Convert the importance dictionary to a DataFrame for easier plotting\n",
    "importance_df = pd.DataFrame(importance.items(), columns=['Feature', 'Importance'])\n",
    "column_names = list(column_names)\n",
    "importance_df['Feature'] = importance_df['Feature'].replace(\n",
    "        {f'f{i}': column_names[i] for i in range(len(column_names))}\n",
    ")\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "top_features = importance_df[:30] if len(importance_df) > 30 else importance_df\n",
    "plt.barh(top_features['Feature'], top_features['Importance'])\n",
    "\n",
    "plt.xlabel('Importance')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to avoid cutting off labels\n",
    "# plt.subplots_adjust(left=0.3)  # Uncomment if tight_layout isn't enough\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cde2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "scaler_name = 'standard'\n",
    "fake_data_length = 0\n",
    "model_type = 'LSTM'\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "tf.random.set_seed(seed_val)\n",
    "data_augmentation = False\n",
    "data = ori_data.copy()\n",
    "n_past = 48 # the ACF shows around 50 lags, so we can use 48 -> 12 hours of lookback\n",
    "n_future = 1\n",
    "\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "\n",
    "# Window sliding technique used for Deep Learning models\n",
    "def split_sequence(X, Y, steps, out):\n",
    "    Xs, Ys = list(), list()\n",
    "    for i in range(len(X)):\n",
    "        end = i + steps\n",
    "        outi = end + out\n",
    "        if outi > len(X)-1:\n",
    "            break\n",
    "        seqx, seqy = X[i:end], Y[end:outi]\n",
    "        Xs.append(seqx)\n",
    "        Ys.append(seqy)\n",
    "    return np.array(Xs), np.array(Ys)\n",
    "\n",
    "# Add time features\n",
    "time_related_features = data[['Timestamp']].copy()      \n",
    "time_related_features['hour_sin'] = np.sin(2 * np.pi * time_related_features['Timestamp'].dt.hour / 24)\n",
    "time_related_features['hour_cos'] = np.cos(2 * np.pi * time_related_features['Timestamp'].dt.hour / 24)\n",
    "time_related_features['day_of_week_sin'] = np.sin(2 * np.pi * time_related_features['Timestamp'].dt.dayofweek / 7)\n",
    "time_related_features['day_of_week_cos'] = np.cos(2 * np.pi * time_related_features['Timestamp'].dt.dayofweek / 7)\n",
    "\n",
    "# Categorical features\n",
    "data.set_index('Timestamp', inplace=True)\n",
    "\n",
    "data['dow'] = data.index.day_name().str[:3]     # e.g. \"Mon\",\"Tue\",…,\"Sun\"\n",
    "data['hour'] = data.index.hour # 0–23\n",
    "sun_low = data['dow'] == 'Sun' # basically all day on sunday\n",
    "mon_low = (data['dow'] == 'Mon') & data['hour'].isin([0,1,2,3,4,5,6,7,8,9]) # basically from 0 to 9 on monday\n",
    "wed_low = (data['dow'] == 'Wed') & data['hour'].isin([6,7,8,9,10,11,12,13,14]) # basically from 6 to 14 for maintenance on wednesday\n",
    "sat_low = (data['dow'] == 'Sat') & data['hour'].isin([19,20,21,22,23]) # basically from 19 to 23 on saturday\n",
    "data['is_low_usage'] = sun_low | mon_low | wed_low | sat_low\n",
    "data.drop(columns=['dow', 'hour'], inplace=True)\n",
    "data.reset_index(inplace=True)\n",
    "data['is_low_usage'] = data['is_low_usage'].astype(int)\n",
    "data['is_low_usage_next'] = data['is_low_usage'].shift(-1)\n",
    "\n",
    "X, Y, X_time= data.drop(columns=['Timestamp']), data['Energy'].values, time_related_features.drop(columns=['Timestamp'])\n",
    "X = pd.concat([X, X_time], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X.values, Y, test_size=0.20, random_state=seed_val, shuffle=False)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.5, random_state=seed_val, shuffle=False)\n",
    "# Split validation data 5% and 5% in order to test in the start and then use the other 5% for the second phase after selection\n",
    "X_val1, X_val2, Y_val1, Y_val2 = train_test_split(X_val, Y_val, test_size=0.5, random_state=seed_val, shuffle=False)\n",
    "# print(X_train.shape, type(X_train), X_val1.shape, type(X_val1))\n",
    "X_train = np.concatenate([X_train, X_val1], axis=0)\n",
    "Y_train = np.concatenate([Y_train, Y_val1], axis=0)\n",
    "X_train, Y_train = split_sequence(X_train, Y_train, n_past, n_future)\n",
    "# X_val1, Y_val1 = split_sequence(X_val1, Y_val1, n_past, n_future)\n",
    "X_test, Y_test = split_sequence(X_test, Y_test, n_past, n_future)\n",
    "X_val2, Y_val2 = split_sequence(X_val2, Y_val2, n_past, n_future)  # Not used in this script\n",
    "\n",
    "print('**** Data shapes ****')\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val1.shape, Y_val1.shape)\n",
    "print(X_val2.shape, Y_val2.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "print('**** Data shapes ****')\n",
    "\n",
    "# train-test split\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=seed_val, shuffle=False)\n",
    "# X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.75, random_state=seed_val, shuffle=False)\n",
    "\n",
    "#-----------Now the data is Train: 80%, Val_1: 5%, Val_2: 5%, Test: 10%----------------#\n",
    "\n",
    "\n",
    "# separate the lagged values, numerical and time features, time features are the last 4 features in the X, lagged value features is the last before the time features\n",
    "# TRAIN\n",
    "X_numerical_train = X_train[:, :, :-7]    # all except last 7 (numerical features)\n",
    "X_lagged_train    = X_train[:, :, -7:-6]  # just the 7th-from-last (lagged feature)\n",
    "X_cat_train       = X_train[:, :, -6:-4]  # just the 6th-from-last (categorical feature)\n",
    "X_time_train      = X_train[:, :, -4:]    # last 4 are time features (time-related features)\n",
    "\n",
    "# VAL\n",
    "X_numerical_val = X_val2[:, :, :-7]\n",
    "X_lagged_val    = X_val2[:, :, -7:-6]\n",
    "X_cat_val       = X_val2[:, :, -6:-4]\n",
    "X_time_val      = X_val2[:, :, -4:]\n",
    "\n",
    "# TEST\n",
    "X_numerical_test = X_test[:, :, :-7]\n",
    "X_lagged_test    = X_test[:, :, -7:-6]\n",
    "X_cat_test       = X_test[:, :, -6:-4]\n",
    "X_time_test      = X_test[:, :, -4:]\n",
    "\n",
    "\n",
    "# broadcast ONLY is_low_usage_next across timesteps\n",
    "# TRAIN\n",
    "X_cat_train[:, :, 1] = X_cat_train[:, -1, 1][:, None]\n",
    "# VAL\n",
    "X_cat_val[:,   :, 1] = X_cat_val[:,   -1, 1][:, None]\n",
    "# TEST\n",
    "X_cat_test[:,  :, 1] = X_cat_test[:,  -1, 1][:, None]\n",
    "\n",
    "if data_augmentation:\n",
    "    fake_data = np.load(\"ddpm_fake_energy_raw.npy\")\n",
    "    fake_data = fake_data[:fake_data_length]\n",
    "    _, seq_len, F = fake_data.shape\n",
    "    X_fake = fake_data[:, :seq_len-1, :]\n",
    "    Y_fake = fake_data[:, seq_len-1, F-1]\n",
    "    Y_fake = Y_fake.reshape(-1, 1)\n",
    "    X_fake_numerical_train = X_fake[:, :, :-6]\n",
    "    X_fake_lagged_train = X_fake[:, :, -6:-5]\n",
    "    X_fake_cat_train = X_fake[:, :, -5:-4]\n",
    "    X_fake_time_train = X_fake[:, :, -4:]\n",
    "\n",
    "if scaler_name == 'min-max':\n",
    "\n",
    "    N, seq_len, F = X_numerical_train.shape\n",
    "    X_numerical_train, scaler = min_max_scaler(X_numerical_train.reshape(-1, X_numerical_train.shape[-1]))\n",
    "    X_numerical_train = X_numerical_train.reshape(N, seq_len, F)\n",
    "    X_numerical_val = scaler.transform(X_numerical_val.reshape(-1, X_numerical_val.shape[-1]))\n",
    "    X_numerical_val = X_numerical_val.reshape(X_val2.shape[0], seq_len, F)\n",
    "    X_numerical_test = scaler.transform(X_numerical_test.reshape(-1, X_numerical_test.shape[-1]))\n",
    "    X_numerical_test = X_numerical_test.reshape(X_test.shape[0], seq_len, F)\n",
    "        \n",
    "    Y_train, scaler_target = min_max_scaler(Y_train)\n",
    "    Y_val = scaler_target.transform(Y_val2)\n",
    "    Y_test = scaler_target.transform(Y_test)\n",
    "    X_lagged_train = scaler_target.transform(X_lagged_train.reshape(-1, X_lagged_train.shape[-1]))\n",
    "    X_lagged_train = X_lagged_train.reshape(N, seq_len, 1)\n",
    "    X_lagged_val = scaler_target.transform(X_lagged_val.reshape(-1, X_lagged_val.shape[-1]))\n",
    "    X_lagged_val = X_lagged_val.reshape(X_val2.shape[0], seq_len, 1)\n",
    "    X_lagged_test = scaler_target.transform(X_lagged_test.reshape(-1, X_lagged_test.shape[-1]))\n",
    "    X_lagged_test = X_lagged_test.reshape(X_test.shape[0], seq_len, 1)\n",
    "\n",
    "\n",
    "    #fake data augmentation\n",
    "    if data_augmentation:\n",
    "        N, seq_len, F = X_fake_numerical_train.shape\n",
    "\n",
    "        X_fake_numerical_train = scaler.transform(X_fake_numerical_train.reshape(-1, X_fake_numerical_train.shape[-1]))\n",
    "        X_fake_numerical_train = X_fake_numerical_train.reshape(N, seq_len, F)\n",
    "        X_fake_lagged_train = scaler_target.transform(X_fake_lagged_train.reshape(-1, X_fake_lagged_train.shape[-1]))\n",
    "        X_fake_lagged_train = X_fake_lagged_train.reshape(N, seq_len, 1)\n",
    "        Y_fake = scaler_target.transform(Y_fake)\n",
    "\n",
    "elif scaler_name == 'standard':\n",
    "\n",
    "    N, seq_len, F = X_numerical_train.shape\n",
    "    X_numerical_train, scaler = standard_scaler(X_numerical_train.reshape(-1, X_numerical_train.shape[-1]))\n",
    "    X_numerical_train = X_numerical_train.reshape(N, seq_len, F)\n",
    "    X_numerical_val = scaler.transform(X_numerical_val.reshape(-1, X_numerical_val.shape[-1]))\n",
    "    X_numerical_val = X_numerical_val.reshape(X_val2.shape[0], seq_len, F)\n",
    "    X_numerical_test = scaler.transform(X_numerical_test.reshape(-1, X_numerical_test.shape[-1]))\n",
    "    X_numerical_test = X_numerical_test.reshape(X_test.shape[0], seq_len, F)\n",
    "\n",
    "    Y_train, scaler_target = standard_scaler(Y_train)\n",
    "    Y_val = scaler_target.transform(Y_val2)\n",
    "    Y_test = scaler_target.transform(Y_test)\n",
    "    X_lagged_train = scaler_target.transform(X_lagged_train.reshape(-1, X_lagged_train.shape[-1]))\n",
    "    X_lagged_train = X_lagged_train.reshape(N, seq_len, 1)\n",
    "    X_lagged_val = scaler_target.transform(X_lagged_val.reshape(-1, X_lagged_val.shape[-1]))\n",
    "    X_lagged_val = X_lagged_val.reshape(X_val2.shape[0], seq_len, 1)\n",
    "    X_lagged_test = scaler_target.transform(X_lagged_test.reshape(-1, X_lagged_test.shape[-1]))\n",
    "    X_lagged_test = X_lagged_test.reshape(X_test.shape[0], seq_len, 1)\n",
    "\n",
    "\n",
    "    #fake data augmentation\n",
    "    if data_augmentation:\n",
    "        N, seq_len, F = X_fake_numerical_train.shape\n",
    "\n",
    "        X_fake_numerical_train = scaler.transform(X_fake_numerical_train.reshape(-1, X_fake_numerical_train.shape[-1]))\n",
    "        X_fake_numerical_train = X_fake_numerical_train.reshape(N, seq_len, F)\n",
    "        X_fake_lagged_train = scaler_target.transform(X_fake_lagged_train.reshape(-1, X_fake_lagged_train.shape[-1]))\n",
    "        X_fake_lagged_train = X_fake_lagged_train.reshape(N, seq_len, 1)\n",
    "        Y_fake = scaler_target.transform(Y_fake)\n",
    "\n",
    "X_train = np.concatenate([X_numerical_train, X_cat_train, X_time_train, X_lagged_train], axis=2)    \n",
    "X_val = np.concatenate([X_numerical_val, X_cat_val, X_time_val, X_lagged_val], axis=2)\n",
    "X_test = np.concatenate([X_numerical_test, X_cat_test, X_time_test, X_lagged_test], axis=2)\n",
    "# X_train = X_numerical_train\n",
    "# X_val = X_numerical_val\n",
    "# X_test = X_numerical_test\n",
    "\n",
    "if model_type == 'BiLSTM':\n",
    "    model = BiLSTM_model(X_train)\n",
    "elif model_type == 'LSTM':\n",
    "    quantiles = [0.05, 0.5, 0.95]\n",
    "    models = {}\n",
    "    histories = {}\n",
    "    for q in quantiles:\n",
    "        print(f\"Training LSTM for quantile {q}...\")\n",
    "        model = LSTM_model(X_train, q)\n",
    "    \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',     # or 'val_mae' depending on preference\n",
    "            patience=20,             # stop after 15 epochs with no improvement\n",
    "            restore_best_weights=True # rollback to the best model weights\n",
    "        )\n",
    "        history = model.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=500, batch_size=16, verbose=1, shuffle=True, callbacks=[early_stop])\n",
    "        models[q] = model\n",
    "        histories[q] = history\n",
    "\n",
    "\n",
    "if data_augmentation:\n",
    "    X_fake = np.concatenate([X_fake_numerical_train, X_fake_cat_train, X_fake_time_train, X_fake_lagged_train], axis=2)\n",
    "    X_train, Y_train = np.concatenate([X_train, X_fake], axis=0), np.concatenate([Y_train, Y_fake], axis=0)\n",
    "\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "predictions = {}\n",
    "for q in quantiles:\n",
    "    predictions[q] = models[q].predict(X_test)\n",
    "\n",
    "# print(X_val.shape)\n",
    "\n",
    "y_pred_q05 = predictions[0.05]\n",
    "y_pred_q50 = predictions[0.5]\n",
    "y_pred_q95 = predictions[0.95]\n",
    "\n",
    "predictions = scaler_target.inverse_transform(y_pred_q50.reshape(-1, 1))\n",
    "y_pred_q05 = scaler_target.inverse_transform(y_pred_q05.reshape(-1, 1))\n",
    "y_pred_q95 = scaler_target.inverse_transform(y_pred_q95.reshape(-1, 1))\n",
    "Y_test = scaler_target.inverse_transform(Y_test)\n",
    "\n",
    "mae = mean_absolute_error(Y_test, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, predictions))\n",
    "r2 = r2_score(Y_test, predictions)\n",
    "n = len(Y_test)\n",
    "# p = shape2\n",
    "# MASE function\n",
    "def mase(y_true, y_pred, y_naive):\n",
    "    # y_true: Actual values\n",
    "    # y_pred: Predicted values\n",
    "    # y_naive: Naive model predictions (e.g., previous period's values)\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_naive = np.asarray(y_naive)\n",
    "    # Compute errors\n",
    "    errors = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Calculate MAE of naive model\n",
    "    naive_errors = np.abs(y_true[1:] - y_naive[1:])\n",
    "    mae_naive = np.mean(naive_errors)\n",
    "    \n",
    "    # Calculate MASE\n",
    "    mase_value = np.mean(errors) / mae_naive\n",
    "    return mase_value\n",
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    # y_true: Actual values\n",
    "    # y_pred: Predicted values\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    # Calculate SMAPE\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    smape_value = 100 * np.mean(diff)\n",
    "    return smape_value\n",
    "smape_val = smape(Y_test, predictions)\n",
    "y_naive = np.roll(Y_test, 1)\n",
    "mase_val = mase(Y_test, predictions, y_naive)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Mean squared error (MSE): {mean_squared_error(Y_test, predictions)}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'R² Score: {r2}')\n",
    "print(f\"SMAPE score: {smape_val}\")\n",
    "print(f\"MASE score: {mase_val}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Plot\n",
    "plt.plot(Y_test.squeeze(), label=\"Ground Truth\", color='black', linewidth=2)\n",
    "plt.plot(predictions.squeeze(), label=\"Median Prediction (0.5)\", color='#0072B2', linewidth=2)\n",
    "plt.fill_between(\n",
    "    np.arange(len(predictions.squeeze())), y_pred_q05.squeeze(), y_pred_q95.squeeze(),\n",
    "    color='#0072B2', alpha=0.2, label=\"90% Confidence Interval (0.05–0.95)\"\n",
    ")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.title(\"XGBoost Quantile Regression - Confidence Interval\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "# wandb.log({\"Prediction vs Actual (train vs validation)\": wandb.Image(plt)})\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
