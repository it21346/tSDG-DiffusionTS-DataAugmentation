{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-FID Score Presentation\n",
    "## Necessary packages and functions call\n",
    "\n",
    "- Context-FID score: A useful metric measures how well the the synthetic time series windows ”fit” into the local context of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), '../'))\n",
    "from Utils.context_fid import Context_FID\n",
    "from Utils.metric_utils import display_scores\n",
    "from Utils.cross_correlation import CrossCorrelLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load original dataset and preprocess the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5\n",
    "dataset_name = 'energy'\n",
    "seq_length = 49\n",
    "# ori_data = np.load('../toy_exp/samples/energy_ground_truth_32_train.npy')\n",
    "ori_data = np.load(f'../energy_results/samples/{dataset_name}_norm_truth_{seq_length}_train.npy')  # Uncomment the line if dataset other than Sine is used.\n",
    "fake_data = np.load('../energy_results/ddpm_fake_energy_0_to_1.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-FID Score\n",
    "\n",
    "- The Frechet Inception distance-like score is based on unsupervised time series embeddings. It is able to score the fit of the fixed length synthetic samples into their context of (often much longer) true time series.\n",
    "\n",
    "- The lowest scoring models correspond to the best performing models in downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0:  context-fid = 0.11474754635038545 \n",
      "\n",
      "Iter 1:  context-fid = 0.11272083864659964 \n",
      "\n",
      "Iter 2:  context-fid = 0.13051291753401065 \n",
      "\n",
      "Iter 3:  context-fid = 0.12563252713605402 \n",
      "\n",
      "Iter 4:  context-fid = 0.12218864509883079 \n",
      "\n",
      "Final Score:  0.12116049495317609 ± 0.00922677670332609\n"
     ]
    }
   ],
   "source": [
    "context_fid_score = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    context_fid = Context_FID(ori_data[:], fake_data[:ori_data.shape[0]])\n",
    "    context_fid_score.append(context_fid)\n",
    "    print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
    "      \n",
    "display_scores(context_fid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlational Score\n",
    "\n",
    "- The metric uses the absolute error of the auto-correlation estimator by real data and synthetic data as the metric to assess the temporal dependency.\n",
    "\n",
    "- For d > 1, it uses the l1-norm of the difference between cross correlation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice(size, num_select=100):\n",
    "    select_idx = np.random.randint(low=0, high=size, size=(num_select,))\n",
    "    return select_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0:  cross-correlation = 4.389076102171767 \n",
      "\n",
      "Iter 1:  cross-correlation = 3.914363407322783 \n",
      "\n",
      "Iter 2:  cross-correlation = 5.643304498817473 \n",
      "\n",
      "Iter 3:  cross-correlation = 6.9368043824817365 \n",
      "\n",
      "Iter 4:  cross-correlation = 5.537986145641662 \n",
      "\n",
      "Final Score:  5.2843069072870845 ± 1.4694404533156824\n"
     ]
    }
   ],
   "source": [
    "x_real = torch.from_numpy(ori_data)\n",
    "x_fake = torch.from_numpy(fake_data)\n",
    "min_len = min(x_real.shape[0], x_fake.shape[0])\n",
    "x_real = x_real[:min_len]\n",
    "x_fake = x_fake[:min_len]\n",
    "assert x_real.shape[2] == x_fake.shape[2], \"Real and fake data must have the same shape except for the batch dimension.\"\n",
    "correlational_score = []\n",
    "size = int(x_real.shape[0] / iterations)\n",
    "\n",
    "for i in range(iterations):\n",
    "    real_idx = random_choice(x_real.shape[0], size)\n",
    "    fake_idx = random_choice(x_fake.shape[0], size)\n",
    "    corr = CrossCorrelLoss(x_real[real_idx, :, :], name='CrossCorrelLoss')\n",
    "    loss = corr.compute(x_fake[fake_idx, :, :])\n",
    "    correlational_score.append(loss.item())\n",
    "    print(f'Iter {i}: ', 'cross-correlation =', loss.item(), '\\n')\n",
    "\n",
    "display_scores(correlational_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Compact metrics saved to ../figures/energy_metrics2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def compute_score(results, confidence=0.95):\n",
    "    \"\"\"Return mean ± CI string like display_scores.\"\"\"\n",
    "    mean = np.mean(results)\n",
    "    sigma = scipy.stats.sem(results)\n",
    "    sigma = sigma * scipy.stats.t.ppf((1 + confidence) / 2., len(results)-1)\n",
    "    return f\"{mean:.4f} ± {sigma:.4f}\"\n",
    "\n",
    "# Example usage after your iterations\n",
    "context_fid_result = compute_score(context_fid_score)\n",
    "corr_result = compute_score(correlational_score)\n",
    "\n",
    "results = {\n",
    "    \"Metric\": [\"Context-FID Score\", \"Correlation Score\"],\n",
    "    \"Result\": [context_fid_result, corr_result]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "save_folder = \"../figures/\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(save_folder, f\"{dataset_name}_metrics2.csv\")\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"✅ Compact metrics saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
